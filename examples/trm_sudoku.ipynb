{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "805491e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict, Union, Any\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.nn.modules.utils import compute_lr\n",
    "from src.nn.utils.constants import IGNORE_LABEL_ID\n",
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "from src.nn.modules.utils import trunc_normal_init_\n",
    "from src.nn.data.sudoku_datamodule import SudokuDataModule\n",
    "\n",
    "from src.nn.modules.utils import stablemax_cross_entropy, trunc_normal_init_\n",
    "from src.nn.utils import RankedLogger\n",
    "\n",
    "from examples.utils import visualize_sudoku_text\n",
    "\n",
    "CosSin = Tuple[torch.Tensor, torch.Tensor]\n",
    "log = RankedLogger(__name__, rank_zero_only=True)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "# set seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bdaa610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CastedLinear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool):\n",
    "        super().__init__()\n",
    "        # Truncated LeCun normal init\n",
    "        self.weight = nn.Parameter(\n",
    "            trunc_normal_init_(\n",
    "                torch.empty((out_features, in_features)), std=1.0 / (in_features**0.5)\n",
    "            )\n",
    "        )\n",
    "        self.bias = None\n",
    "        if bias:\n",
    "            # Zero init bias\n",
    "            self.bias = nn.Parameter(torch.zeros((out_features,)))\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return F.linear(\n",
    "            input,\n",
    "            self.weight.to(input.dtype),\n",
    "            bias=self.bias.to(input.dtype) if self.bias is not None else None,\n",
    "        )\n",
    "    \n",
    "class CastedEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_embeddings: int, embedding_dim: int, init_std: float, cast_to: torch.dtype\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cast_to = cast_to\n",
    "\n",
    "        # Truncated LeCun normal init\n",
    "        self.embedding_weight = nn.Parameter(\n",
    "            trunc_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=init_std)\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return F.embedding(input, self.embedding_weight.to(self.cast_to))\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, hidden_size: int, expansion: float):\n",
    "        super().__init__()\n",
    "        inter = _find_multiple(round(expansion * hidden_size * 2 / 3), 256)\n",
    "\n",
    "        self.gate_up_proj = CastedLinear(hidden_size, inter * 2, bias=False)\n",
    "        self.down_proj = CastedLinear(inter, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate, up = self.gate_up_proj(x).chunk(2, dim=-1)\n",
    "        return self.down_proj(F.silu(gate) * up)\n",
    "\n",
    "def rms_norm(hidden_states: torch.Tensor, variance_epsilon: float) -> torch.Tensor:\n",
    "    input_dtype = hidden_states.dtype\n",
    "    hidden_states = hidden_states.to(torch.float32)\n",
    "\n",
    "    variance = hidden_states.square().mean(-1, keepdim=True)\n",
    "    hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
    "    return hidden_states.to(input_dtype)\n",
    "\n",
    "def _find_multiple(a, b):\n",
    "    return (-(a // -b)) * b\n",
    "\n",
    "class ReasoningBlockConfig:\n",
    "    \"\"\"\n",
    "    Configuration A: The \"Standard Transformer\"\n",
    "        mlp_t=False (Use Attention)\n",
    "        use_convswiglu/use_boardswiglu=False (Use Standard MLP)\n",
    "        Result: Classic powerful reasoning. Attention handles global context; MLP handles logic.\n",
    "\n",
    "    Configuration B: The \"Spatial-Inductive Transformer\"\n",
    "        mlp_t=False (Use Attention)\n",
    "        use_convswiglu/use_boardswiglu=True (Use Conv MLP)\n",
    "        Result: Strongest. Attention sees the whole board (\"I can win in column 7\"), while ConvSwiGLU recognizes patterns immediately (\"I have 3-in-a-row here\"). This gives the best of both worlds.\n",
    "\n",
    "    Configuration C: The \"MLP-Mixer\" (Pure MLP)\n",
    "        mlp_t=True (Use Token MLP)\n",
    "        use_convswiglu/use_boardswiglu=False (Use Standard MLP)\n",
    "        Result: Very fast, very stable, but no Attention. The model mixes information globally using a fixed matrix. It might struggle with \"dynamic\" reasoning.\n",
    "\n",
    "    Configuration D: The \"ConvMixer\"\n",
    "        mlp_t=True\n",
    "        use_convswiglu/use_boardswiglu=True\n",
    "        Result: A fully convolutional/MLP network. It has zero attention mechanisms: mlp_t mixes the board globally (fixed weights), convswiglu mixes neighbors locally.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_heads: int,\n",
    "        expansion: int,\n",
    "        rms_norm_eps: float,\n",
    "        mlp_t: bool = False,\n",
    "        seq_len: int = 0,\n",
    "        cols: int = None,\n",
    "        rows: int = None,\n",
    "        puzzle_emb_ndim: int = 0,\n",
    "        puzzle_emb_len: int = 0,\n",
    "        use_conv_swiglu: bool = False,\n",
    "        use_board_swiglu: bool = False,\n",
    "        dropout: float = 0.1\n",
    "    ) -> None:\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.expansion = expansion\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.mlp_t = mlp_t\n",
    "        self.puzzle_emb_ndim = puzzle_emb_ndim\n",
    "        self.puzzle_emb_len = puzzle_emb_len\n",
    "        self.seq_len = seq_len\n",
    "        self.cols = cols\n",
    "        self.rows = rows\n",
    "        self.use_conv_swiglu = use_conv_swiglu\n",
    "        self.use_board_swiglu = use_board_swiglu\n",
    "        self.dropout = dropout\n",
    "\n",
    "class ReasoningBlock(nn.Module):\n",
    "    def __init__(self, config: ReasoningBlockConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.norm_eps = config.rms_norm_eps\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # 1. Calculate Effective Length\n",
    "        # If config is 0 (auto), infer from dimensions. Otherwise use config.\n",
    "        # This handles the case where puzzle_emb_ndim > 0 but puzzle_emb_len was not manually set.\n",
    "        self.puzzle_emb_len = (\n",
    "            -(config.puzzle_emb_ndim // -config.hidden_size)\n",
    "            if config.puzzle_emb_len == 0\n",
    "            else config.puzzle_emb_len\n",
    "        )\n",
    "\n",
    "        self.mlp_t = SwiGLU(\n",
    "            hidden_size=config.seq_len + self.puzzle_emb_len, \n",
    "            expansion=config.expansion,\n",
    "        )\n",
    "        self.mlp = SwiGLU(\n",
    "            hidden_size=config.hidden_size,\n",
    "            expansion=config.expansion,\n",
    "        )\n",
    "\n",
    "    def forward(self, cos_sin: CosSin, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        if self.config.mlp_t:\n",
    "            hidden_states = hidden_states.transpose(1,2)\n",
    "            out = self.mlp_t(hidden_states)\n",
    "            out = self.dropout(out)\n",
    "            hidden_states = rms_norm(hidden_states + out, variance_epsilon=self.norm_eps)\n",
    "            hidden_states = hidden_states.transpose(1,2)\n",
    "        else:\n",
    "            attn_out = self.self_attn(cos_sin=cos_sin, hidden_states=hidden_states)\n",
    "            hidden_states = rms_norm(hidden_states + attn_out, variance_epsilon=self.norm_eps)\n",
    "\n",
    "        mlp_out = self.mlp(hidden_states)\n",
    "        hidden_states = rms_norm(hidden_states + mlp_out, variance_epsilon=self.norm_eps)\n",
    "            \n",
    "        return hidden_states\n",
    "\n",
    "class ReasoningModule(nn.Module):\n",
    "    def __init__(self, layers: List[ReasoningBlock]):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states: torch.Tensor, input_injection: torch.Tensor, **kwargs\n",
    "    ) -> torch.Tensor:\n",
    "        hidden_states = hidden_states + input_injection\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states=hidden_states, **kwargs)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31cbd668",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TRMInnerCarry:\n",
    "    z_H: torch.Tensor  # High-level state (y = the solution representation)\n",
    "    z_L: torch.Tensor  # Low-level state (z = the problem representation)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TRMCarry:\n",
    "    \"\"\"Carry structure for maintaining state across steps.\"\"\"\n",
    "\n",
    "    inner_carry: TRMInnerCarry\n",
    "    steps: torch.Tensor\n",
    "    halted: torch.Tensor\n",
    "    current_data: Dict[str, torch.Tensor]  # Stores current batch data\n",
    "    \n",
    "class TRMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    HRM implementation following Figure 2 pseudocode exactly.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 512,\n",
    "        num_layers: int = 2,\n",
    "        num_heads: int = 8,  # min(2, hidden_size // 64)\n",
    "        max_grid_size: int = 30,\n",
    "        H_cycles: int = 3,\n",
    "        L_cycles: int = 6,\n",
    "        N_supervision: int = 16,\n",
    "        N_supervision_val: int = 16,\n",
    "        ffn_expansion: int = 2,\n",
    "        learning_rate: float = 1e-4,\n",
    "        learning_rate_emb: float = 1e-2,\n",
    "        weight_decay: float = 0.01,\n",
    "        warmup_steps: int = 2000,\n",
    "        halt_exploration_prob: float = 0.1,\n",
    "        puzzle_emb_dim: int = 512,  # Puzzle embedding dimension\n",
    "        puzzle_emb_len: int = 16,  # How many tokens for puzzle embedding\n",
    "        rope_theta: int = 10000,\n",
    "        pos_emb_type: str = \"1d\",\n",
    "        use_mlp_t: bool = False,\n",
    "        use_conv_swiglu: bool = False,\n",
    "        use_board_swiglu: bool = False,\n",
    "        lr_min_ratio: float = 1.0,\n",
    "        use_muon: bool = False,\n",
    "        vocab_size: int = 0,  # Should be set from datamodule\n",
    "        num_puzzles: int = 0,  # Should be set from datamodule\n",
    "        batch_size: int = 0,  # Should be set from datamodule\n",
    "        pad_value: int = -1,  # Should be set from datamodule\n",
    "        seq_len: int = 0,  # Should be set from datamodule\n",
    "        output_dir: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.num_heads=num_heads\n",
    "        self.max_grid_size=max_grid_size\n",
    "        self.H_cycles=H_cycles\n",
    "        self.L_cycles=L_cycles\n",
    "        self.N_supervision=N_supervision\n",
    "        self.N_supervision_val=N_supervision_val\n",
    "        self.ffn_expansion=ffn_expansion\n",
    "        self.learning_rate=learning_rate\n",
    "        self.learning_rate_emb=learning_rate_emb\n",
    "        self.weight_decay=weight_decay\n",
    "        self.warmup_steps=warmup_steps\n",
    "        self.halt_exploration_prob=halt_exploration_prob\n",
    "        self.puzzle_emb_dim=puzzle_emb_dim\n",
    "        self.puzzle_emb_len=puzzle_emb_len\n",
    "        self.rope_theta=rope_theta\n",
    "        self.pos_emb_type=pos_emb_type\n",
    "        self.use_mlp_t=use_mlp_t\n",
    "        self.use_conv_swiglu=use_conv_swiglu\n",
    "        self.use_board_swiglu=use_board_swiglu\n",
    "        self.lr_min_ratio=lr_min_ratio\n",
    "        self.use_muon=use_muon\n",
    "        self.vocab_size=vocab_size\n",
    "        self.num_puzzles=num_puzzles\n",
    "        self.batch_size=batch_size\n",
    "        self.pad_value=pad_value\n",
    "        self.seq_len=seq_len\n",
    "        self.output_dir=output_dir\n",
    "        self.forward_dtype = torch.bfloat16\n",
    "\n",
    "        # Token embeddings\n",
    "        self.embed_scale = math.sqrt(hidden_size)\n",
    "        embed_init_std = 1.0 / self.embed_scale\n",
    "\n",
    "        log.info(f\"Creating TRM with vocab size={vocab_size}, seq_len={seq_len}, puzzle_emb_len={puzzle_emb_len} {pos_emb_type=} {puzzle_emb_dim=}\")\n",
    "        log.info(f\"{use_mlp_t=}, {use_conv_swiglu=}, {use_board_swiglu=}\")\n",
    "\n",
    "         # Input embedding\n",
    "        self.input_embedding = CastedEmbedding(\n",
    "            vocab_size, hidden_size, init_std=embed_init_std, cast_to=self.forward_dtype\n",
    "        )\n",
    "\n",
    "        # Positional encoding\n",
    "        log.info(\"Not using Rotary Embeddings\")\n",
    "\n",
    "        if not use_mlp_t:\n",
    "            assert pos_emb_type is not None, \"Rotary embeddings required if using attention\"\n",
    "\n",
    "        # a single network (not two separate networks)\n",
    "        reasoning_config = ReasoningBlockConfig(\n",
    "            hidden_size=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            expansion=ffn_expansion,\n",
    "            rms_norm_eps=1e-5,\n",
    "            seq_len=seq_len,\n",
    "            mlp_t=use_mlp_t,\n",
    "            puzzle_emb_ndim=puzzle_emb_dim,\n",
    "            puzzle_emb_len=puzzle_emb_len,\n",
    "            use_conv_swiglu=use_conv_swiglu,\n",
    "            use_board_swiglu=use_board_swiglu,\n",
    "            rows = max_grid_size,\n",
    "            cols = max_grid_size,\n",
    "            dropout=0.25\n",
    "        )\n",
    "\n",
    "        self.lenet = ReasoningModule(\n",
    "            layers=[ReasoningBlock(reasoning_config) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.lm_head = CastedLinear(hidden_size, vocab_size, bias=False)\n",
    "        self.q_head = CastedLinear(hidden_size, 1, bias=True) # learn to stop, not to continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.q_head.weight.zero_()\n",
    "            if self.q_head.bias is not None:\n",
    "                self.q_head.bias.fill_(-5.0)  # Strong negative bias\n",
    "\n",
    "        # State for carry (persisted across training steps)\n",
    "        self.carry: Optional[TRMCarry] = None\n",
    "\n",
    "        # Init states (registered buffers)\n",
    "        self.register_buffer(\n",
    "            \"z_H_init\",\n",
    "            trunc_normal_init_(torch.empty(hidden_size, dtype=self.forward_dtype), std=1),\n",
    "            persistent=True,\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"z_L_init\",\n",
    "            trunc_normal_init_(torch.empty(hidden_size, dtype=self.forward_dtype), std=1),\n",
    "            persistent=True,\n",
    "        )\n",
    "\n",
    "        log.info(\"puzzle_emb_dim <= 0, not creating puzzle embeddings\")\n",
    "        self.puzzle_emb = None\n",
    "        self.puzzle_emb_len = 0\n",
    "\n",
    "        self.manual_step = 0\n",
    "        self.total_steps: float = float(\"inf\")\n",
    "\n",
    "    def _input_embeddings(self, input: torch.Tensor, puzzle_identifiers: torch.Tensor):\n",
    "        # Token embedding\n",
    "        embedding = self.input_embedding(input.to(torch.int32))\n",
    "\n",
    "        # Puzzle embeddings\n",
    "        if self.puzzle_emb_dim > 0:\n",
    "            puzzle_embedding = self.puzzle_emb(puzzle_identifiers)\n",
    "\n",
    "            pad_count = self.puzzle_emb_len * self.hidden_size - puzzle_embedding.shape[-1]\n",
    "\n",
    "            if pad_count > 0:\n",
    "                puzzle_embedding = F.pad(puzzle_embedding, (0, pad_count))\n",
    "\n",
    "            embedding = torch.cat(\n",
    "                (\n",
    "                    puzzle_embedding.view(-1, self.puzzle_emb_len, self.hidden_size),\n",
    "                    embedding,\n",
    "                ),\n",
    "                dim=-2,\n",
    "            )\n",
    "\n",
    "        # Scale\n",
    "        return self.embed_scale * embedding\n",
    "\n",
    "    def initial_carry(self, batch: Dict[str, torch.Tensor]):\n",
    "        batch_size = batch[\"input\"].shape[0]\n",
    "        device = batch[\"input\"].device\n",
    "\n",
    "        return TRMCarry(\n",
    "            inner_carry=self.empty_carry(\n",
    "                batch_size, device\n",
    "            ),  # Empty is expected, it will be reseted in first pass as all sequences are halted.\n",
    "            steps=torch.zeros((batch_size,), dtype=torch.int32, device=device),\n",
    "            halted=torch.ones((batch_size,), dtype=torch.bool, device=device),  # Default to halted\n",
    "            current_data={k: torch.empty_like(v, device=device) for k, v in batch.items()},\n",
    "        )\n",
    "\n",
    "    def empty_carry(self, batch_size: int, device: torch.device) -> TRMInnerCarry:\n",
    "        return TRMInnerCarry(\n",
    "            z_H=torch.empty(\n",
    "                batch_size,\n",
    "                self.seq_len + self.puzzle_emb_len,\n",
    "                self.hidden_size,\n",
    "                dtype=self.forward_dtype,\n",
    "                device=device,\n",
    "            ),\n",
    "            z_L=torch.empty(\n",
    "                batch_size,\n",
    "                self.seq_len + self.puzzle_emb_len,\n",
    "                self.hidden_size,\n",
    "                dtype=self.forward_dtype,\n",
    "                device=device,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def reset_carry(self, reset_flag: torch.Tensor, carry: TRMInnerCarry) -> TRMInnerCarry:\n",
    "        return TRMInnerCarry(\n",
    "            z_H=torch.where(reset_flag.view(-1, 1, 1), self.z_H_init, carry.z_H),\n",
    "            z_L=torch.where(reset_flag.view(-1, 1, 1), self.z_L_init, carry.z_L),\n",
    "        )\n",
    "\n",
    "    def inner_forward(\n",
    "        self, carry: TRMInnerCarry, batch: Dict[str, torch.Tensor]\n",
    "    ) -> Tuple[TRMInnerCarry, torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        seq_info = dict(\n",
    "            cos_sin=self.pos_embedding() if hasattr(self, \"pos_embedding\") else None,\n",
    "        )\n",
    "\n",
    "        # Input encoding\n",
    "        input_embeddings = self._input_embeddings(batch[\"input\"], batch[\"puzzle_identifiers\"])\n",
    "\n",
    "        # Forward iterations\n",
    "        z_H, z_L = carry.z_H, carry.z_L\n",
    "        # H_cycles-1 without grad\n",
    "        with torch.no_grad():\n",
    "            for _ in range(self.H_cycles - 1):\n",
    "                for _ in range(self.L_cycles):\n",
    "                    z_L = self.lenet(z_L, z_H + input_embeddings, **seq_info)\n",
    "                z_H = self.lenet(z_H, z_L, **seq_info)\n",
    "        # 1 with grad\n",
    "        for _ in range(self.L_cycles):\n",
    "            z_L = self.lenet(z_L, z_H + input_embeddings, **seq_info)\n",
    "        z_H = self.lenet(z_H, z_L, **seq_info)\n",
    "    \n",
    "        # LM Outputs\n",
    "        new_carry = TRMInnerCarry(z_H=z_H.detach(), z_L=z_L.detach())  # New carry no grad\n",
    "        output = self.lm_head(z_H)[:, self.puzzle_emb_len :] # discard puzzle embeddings\n",
    "        q_logits = self.q_head(z_H[:, 0]).to(\n",
    "            torch.float32\n",
    "        )  # Q-head; uses the first puzzle_emb position\n",
    "\n",
    "        return new_carry, output, q_logits[..., 0]\n",
    "\n",
    "    def forward(\n",
    "        self, carry: TRMCarry, batch: Dict[str, torch.Tensor]\n",
    "    ) -> Tuple[TRMCarry, Dict[str, torch.Tensor]]:\n",
    "        # Update data, carry (removing halted sequences)\n",
    "        new_inner_carry = self.reset_carry(carry.halted, carry.inner_carry)\n",
    "\n",
    "        new_steps = torch.where(carry.halted, 0, carry.steps)\n",
    "\n",
    "        new_current_data = {\n",
    "            k: torch.where(carry.halted.view((-1,) + (1,) * (batch[k].ndim - 1)), batch[k], v)\n",
    "            for k, v in carry.current_data.items()\n",
    "        }\n",
    "\n",
    "        # Forward inner model\n",
    "        new_inner_carry, logits, q_halt_logits = self.inner_forward(\n",
    "            new_inner_carry, new_current_data\n",
    "        )\n",
    "\n",
    "        outputs = {\n",
    "            \"logits\": logits,\n",
    "            \"q_halt_logits\": q_halt_logits,\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Step\n",
    "            new_steps = new_steps + 1\n",
    "            n_supervision_steps = (\n",
    "                self.N_supervision if self.training else self.N_supervision_val\n",
    "            )\n",
    "\n",
    "            is_last_step = new_steps >= n_supervision_steps\n",
    "\n",
    "            halted = is_last_step\n",
    "\n",
    "            # if training, and ACT is enabled\n",
    "            if self.training and (self.N_supervision > 1):\n",
    "                # Halt signal\n",
    "                # NOTE: During evaluation, always use max steps, this is to guarantee the same halting steps inside a batch for batching purposes\n",
    "\n",
    "                halted = halted | (q_halt_logits > 0)\n",
    "\n",
    "                # Exploration\n",
    "                min_halt_steps = (\n",
    "                    torch.rand_like(q_halt_logits) < self.halt_exploration_prob\n",
    "                ) * torch.randint_like(new_steps, low=2, high=self.N_supervision + 1)\n",
    "                halted = halted & (new_steps >= min_halt_steps)\n",
    "\n",
    "        return TRMCarry(new_inner_carry, new_steps, halted, new_current_data), outputs\n",
    "\n",
    "    def compute_loss_and_metrics(self, carry, batch):\n",
    "        \"\"\"Compute loss and metrics without circular reference.\"\"\"\n",
    "        # Get model outputs\n",
    "        new_carry, outputs = self.forward(carry, batch)\n",
    "        labels = new_carry.current_data[\"output\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs[\"preds\"] = torch.argmax(outputs[\"logits\"], dim=-1)\n",
    "\n",
    "            # Correctness\n",
    "            mask = labels != IGNORE_LABEL_ID\n",
    "            loss_counts = mask.sum(-1)\n",
    "\n",
    "            loss_divisor = loss_counts.clamp_min(1).unsqueeze(-1)  # Avoid NaNs in division\n",
    "\n",
    "            is_correct = mask & (torch.argmax(outputs[\"logits\"], dim=-1) == labels)\n",
    "            seq_is_correct = is_correct.sum(-1) == loss_counts\n",
    "\n",
    "            # Metrics (halted)\n",
    "            valid_metrics = new_carry.halted & (loss_counts > 0)\n",
    "\n",
    "            metrics = {\n",
    "                \"count\": valid_metrics.sum(),\n",
    "                \"accuracy\": torch.where(\n",
    "                    valid_metrics, (is_correct.float() / loss_divisor).sum(-1), 0\n",
    "                ).sum(),\n",
    "                \"exact_accuracy\": (valid_metrics & seq_is_correct).sum(),\n",
    "                \"q_halt_accuracy\": (\n",
    "                    valid_metrics & ((outputs[\"q_halt_logits\"].squeeze() >= 0) == seq_is_correct)\n",
    "                ).sum(),\n",
    "                \"steps\": torch.where(valid_metrics, new_carry.steps, 0).sum(),\n",
    "            }\n",
    "\n",
    "        # Compute losses: These are per-sequence losses that will be summed\n",
    "        lm_loss = (\n",
    "            stablemax_cross_entropy(\n",
    "                outputs[\"logits\"], labels, ignore_index=IGNORE_LABEL_ID, valid_mask=mask\n",
    "            )\n",
    "            / loss_divisor\n",
    "        ).sum()\n",
    "\n",
    "        q_halt_loss = F.binary_cross_entropy_with_logits(\n",
    "            outputs[\"q_halt_logits\"],\n",
    "            seq_is_correct.to(outputs[\"q_halt_logits\"].dtype),\n",
    "            reduction=\"sum\",\n",
    "        )\n",
    "        metrics.update(\n",
    "            {\n",
    "                \"lm_loss\": lm_loss.detach(),\n",
    "                \"q_halt_loss\": q_halt_loss.detach(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        total_loss = lm_loss + 0.5 * q_halt_loss\n",
    "\n",
    "        return new_carry, total_loss, metrics, new_carry.halted.all()\n",
    "\n",
    "    def log_metrics(self, metrics: dict, lr_this_step: float = None, batch_size: int = None):\n",
    "\n",
    "        # Log learning rate (will log the last optimizer's LR)\n",
    "        self.log(\"train/lr\", lr_this_step, on_step=True)\n",
    "\n",
    "        # Log metrics\n",
    "        if metrics.get(\"count\", 0) > 0:\n",
    "            with torch.no_grad():\n",
    "                count = metrics[\"count\"]\n",
    "                self.log(\"train/accuracy\", metrics.get(\"accuracy\", 0) / count, on_step=True)\n",
    "                self.log(\n",
    "                    \"train/exact_accuracy\",\n",
    "                    metrics.get(\"exact_accuracy\", 0) / count,\n",
    "                    prog_bar=True,\n",
    "                    on_step=True,\n",
    "                )\n",
    "                self.log(\n",
    "                    \"train/q_halt_accuracy\",\n",
    "                    metrics.get(\"q_halt_accuracy\", 0) / count,\n",
    "                    on_step=True,\n",
    "                )\n",
    "                self.log(\n",
    "                    \"train/steps\",\n",
    "                    metrics.get(\"steps\", 0) / count,\n",
    "                    prog_bar=True,\n",
    "                    on_step=True,\n",
    "                )\n",
    "\n",
    "                self.log(\"train/lm_loss\", metrics.get(\"lm_loss\", 0) / batch_size, on_step=True)\n",
    "                self.log(\n",
    "                    \"train/q_halt_loss\", metrics.get(\"q_halt_loss\", 0) / batch_size, on_step=True\n",
    "                )\n",
    "\n",
    "                avg_halt_steps = metrics.get(\"steps\", 0) / metrics[\"count\"]\n",
    "                early_halt_rate = avg_halt_steps < self.N_supervision\n",
    "                self.log(\"train/early_halt_rate\", early_halt_rate, on_step=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990a696",
   "metadata": {},
   "source": [
    "### Load sudoku data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55bafa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Ready: 6x6 Sudoku\n",
      "Vocab Size: 9, Sequence Length: 64\n",
      "tensor([[ 2,  2,  7,  2,  9,  8],\n",
      "        [ 2,  9,  2,  2,  4,  3],\n",
      "        [ 2, 10,  2,  8,  2,  2],\n",
      "        [ 9,  2,  8,  2, 10,  2],\n",
      "        [10,  8,  4,  9,  2,  2],\n",
      "        [ 2,  3,  2,  4,  8, 10]])\n",
      "============================================================\n",
      "Sample 0 (grid_size=6x6)\n",
      "============================================================\n",
      "Givens: 19, Empty: 17\n",
      "\n",
      "Puzzle:\n",
      "+-------+-------+\n",
      "| . . 5 | . 7 6 |\n",
      "| . 7 . | . 2 1 |\n",
      "+-------+-------+\n",
      "| . 8 . | 6 . . |\n",
      "| 7 . 6 | . 8 . |\n",
      "+-------+-------+\n",
      "| 8 6 2 | 7 . . |\n",
      "| . 1 . | 2 6 8 |\n",
      "+-------+-------+\n",
      "\n",
      "Solution:\n",
      "+-------+-------+\n",
      "| 1 2 5 | 8 7 6 |\n",
      "| 6 7 8 | 5 2 1 |\n",
      "+-------+-------+\n",
      "| 2 8 1 | 6 5 7 |\n",
      "| 7 5 6 | 1 8 2 |\n",
      "+-------+-------+\n",
      "| 8 6 2 | 7 1 5 |\n",
      "| 5 1 7 | 2 6 8 |\n",
      "+-------+-------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_size = 6\n",
    "max_grid_size = 8\n",
    "dm = SudokuDataModule(\n",
    "    data_dir=None,       \n",
    "    batch_size=32,       \n",
    "    num_train_puzzles=1000,\n",
    "    num_val_puzzles=100,\n",
    "    num_test_puzzles=100,\n",
    "    grid_size=grid_size,\n",
    "    max_grid_size=max_grid_size,\n",
    "    num_workers=0,\n",
    "    seed=42\n",
    ")\n",
    "dm.setup()\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "\n",
    "vocab_size = dm.vocab_size \n",
    "seq_len = dm.seq_len\n",
    "puzzle_emb_len = 0\n",
    "\n",
    "print(f\"Data Ready: {grid_size}x{grid_size} Sudoku\")\n",
    "print(f\"Vocab Size: {vocab_size}, Sequence Length: {seq_len}\")\n",
    "batch = next(iter(train_loader))\n",
    "# Visualize the first element in the batch\n",
    "inp = batch['input'][0].reshape(max_grid_size, max_grid_size)\n",
    "tgt = batch['output'][0].reshape(max_grid_size, max_grid_size)\n",
    "inp_grid = inp[:grid_size, :grid_size]\n",
    "tgt_grid = tgt[:grid_size, :grid_size]\n",
    "print(inp_grid)\n",
    "visualize_sudoku_text(batch, idx=0, grid_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4163b0cb",
   "metadata": {},
   "source": [
    "### Define TRM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "577a2301",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TRMModel(\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    num_heads=8,\n",
    "    max_grid_size=6,\n",
    "    H_cycles=3,\n",
    "    L_cycles=6,\n",
    "    N_supervision=16,\n",
    "    N_supervision_val=16,\n",
    "    ffn_expansion=4,\n",
    "    learning_rate=1e-4,\n",
    "    learning_rate_emb=1e-4,\n",
    "    weight_decay=1.0,\n",
    "    warmup_steps=2000,\n",
    "    halt_exploration_prob=0.1,\n",
    "    puzzle_emb_dim=0,\n",
    "    puzzle_emb_len=0,\n",
    "    rope_theta=10000,\n",
    "    pos_emb_type=None, # IMPORTANT: since use_mlp_t=True\n",
    "    use_mlp_t=True,\n",
    "    use_conv_swiglu=False,\n",
    "    use_board_swiglu=False,\n",
    "    lr_min_ratio=0.01,\n",
    "    use_muon=False,\n",
    "    vocab_size=3+dm.max_grid_size,\n",
    "    num_puzzles=0,\n",
    "    batch_size=train_loader.batch_size,\n",
    "    pad_value=getattr(dm, \"pad_value\", 0),\n",
    "    seq_len=seq_len,\n",
    "    output_dir=None,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f393ae6",
   "metadata": {},
   "source": [
    "### Define TRM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fffc3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = model.learning_rate\n",
    "wd = model.weight_decay\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=base_lr,\n",
    "    weight_decay=wd,\n",
    "    betas=(0.9, 0.95),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49af2d8",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c8b8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_batch_to_device(batch, device):\n",
    "    return {k: (v.to(device, non_blocking=True) if torch.is_tensor(v) else v) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f73f32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, carry, batch, optimizer, *, total_steps, grad_clip=1.0):\n",
    "    \"\"\"\n",
    "    Mirrors your Lightning training_step, but as a pure function.\n",
    "    - one forward pass per batch (carry persists across batches)\n",
    "    - manual backward\n",
    "    - warmup LR schedule\n",
    "    - gradient clipping\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    batch_size = batch[\"input\"].shape[0]\n",
    "\n",
    "    if carry is None:\n",
    "        carry = model.initial_carry(batch)\n",
    "\n",
    "    carry, loss, metrics, _ = model.compute_loss_and_metrics(carry, batch)\n",
    "\n",
    "    (loss / batch_size).backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "\n",
    "    # Warmup LR schedule (same logic)\n",
    "    current_step = model.manual_step\n",
    "\n",
    "    last_lr = None\n",
    "    if current_step < model.warmup_steps:\n",
    "        lr_this_step = compute_lr(\n",
    "            base_lr=model.learning_rate,\n",
    "            lr_warmup_steps=model.warmup_steps,\n",
    "            lr_min_ratio=model.lr_min_ratio,\n",
    "            current_step=current_step,\n",
    "            total_steps=total_steps,\n",
    "        )\n",
    "    else:\n",
    "        lr_this_step = model.learning_rate\n",
    "\n",
    "    if hasattr(optimizer, \"_optimizer\"):\n",
    "        for param_group in optimizer._optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr_this_step\n",
    "            optimizer._optimizer.step()\n",
    "            optimizer._optimizer.zero_grad()\n",
    "        else:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr_this_step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    # safety\n",
    "    if torch.isnan(metrics[\"lm_loss\"]):\n",
    "        raise RuntimeError(f\"LM loss is NaN at step {model.manual_step}\")\n",
    "\n",
    "    model.manual_step += 1\n",
    "\n",
    "    # return carry + some printable scalars\n",
    "    count = float(metrics[\"count\"].item())\n",
    "    logs = {\n",
    "        \"train/loss\": float(loss.item()),\n",
    "        \"train/lm_loss\": float((metrics[\"lm_loss\"] / batch_size).item()),\n",
    "        \"train/q_halt_loss\": float((metrics[\"q_halt_loss\"] / batch_size).item()),\n",
    "        \"train/lr\": float(last_lr) if last_lr is not None else float(\"nan\"),\n",
    "    }\n",
    "    if count > 0:\n",
    "        logs.update(\n",
    "            {\n",
    "                \"train/accuracy\": float((metrics[\"accuracy\"] / metrics[\"count\"]).item()),\n",
    "                \"train/exact_accuracy\": float((metrics[\"exact_accuracy\"] / metrics[\"count\"]).item()),\n",
    "                \"train/q_halt_accuracy\": float((metrics[\"q_halt_accuracy\"] / metrics[\"count\"]).item()),\n",
    "                \"train/steps\": float((metrics[\"steps\"] / metrics[\"count\"]).item()),\n",
    "            }\n",
    "        )\n",
    "    return carry, loss, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a712438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  2, 10,  2,  7,  3,  0,  0],\n",
      "        [ 2,  7,  2,  2,  4,  6,  0,  0],\n",
      "        [ 2,  5,  2,  3,  2,  2,  0,  0],\n",
      "        [ 7,  2,  3,  2,  5,  2,  0,  0],\n",
      "        [ 5,  3,  4,  7,  2,  2,  0,  0],\n",
      "        [ 2,  6,  2,  4,  3,  5,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 000 | batch 0000 | loss 82.0539 | exact nan | steps nan | lr nan\n",
      "tensor([[ 2,  2,  7,  5, 10,  2,  0,  0],\n",
      "        [ 2, 10,  2,  3,  8,  2,  0,  0],\n",
      "        [ 2,  2,  2,  9,  2,  8,  0,  0],\n",
      "        [ 9,  2,  8,  7,  3, 10,  0,  0],\n",
      "        [ 8,  9,  2,  2,  7,  5,  0,  0],\n",
      "        [10,  7,  2,  2,  9,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2,  3, 10,  2,  5,  2,  0,  0],\n",
      "        [ 2,  2,  5,  6, 10,  2,  0,  0],\n",
      "        [ 2,  2,  8,  2,  3,  2,  0,  0],\n",
      "        [ 5,  2,  2,  2,  8,  6,  0,  0],\n",
      "        [ 2, 10,  9,  3,  6,  5,  0,  0],\n",
      "        [ 3,  5,  6,  2,  2, 10,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 7,  2,  5,  9,  2,  4,  0,  0],\n",
      "        [ 2,  2,  4,  2,  7,  2,  0,  0],\n",
      "        [ 2,  2,  7,  2, 10,  6,  0,  0],\n",
      "        [10,  2,  2,  5,  2,  2,  0,  0],\n",
      "        [ 5,  2,  2,  2,  4,  2,  0,  0],\n",
      "        [ 2,  7,  9,  6,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 5,  8, 10,  3,  9,  2,  0,  0],\n",
      "        [ 9,  2,  2,  2,  5,  2,  0,  0],\n",
      "        [ 2,  2,  2,  2,  2,  2,  0,  0],\n",
      "        [ 2,  5,  2,  2,  2,  2,  0,  0],\n",
      "        [ 8,  2,  2,  2,  2,  2,  0,  0],\n",
      "        [ 6,  2,  5,  2,  8,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 7,  2,  9,  3,  6,  2,  0,  0],\n",
      "        [ 2,  2,  2, 10,  9,  7,  0,  0],\n",
      "        [ 3,  9,  2,  6,  7,  2,  0,  0],\n",
      "        [10,  6,  2,  9,  4,  3,  0,  0],\n",
      "        [ 2,  4,  2,  7,  3,  2,  0,  0],\n",
      "        [ 9,  2,  2,  2,  2,  6,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[2, 2, 2, 8, 2, 3, 0, 0],\n",
      "        [2, 2, 2, 2, 2, 5, 0, 0],\n",
      "        [5, 8, 2, 3, 7, 9, 0, 0],\n",
      "        [3, 2, 9, 5, 2, 6, 0, 0],\n",
      "        [2, 5, 2, 6, 3, 2, 0, 0],\n",
      "        [6, 3, 8, 9, 5, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[ 2,  2,  2,  2,  5,  2,  0,  0],\n",
      "        [ 2,  2,  3, 10,  2,  4,  0,  0],\n",
      "        [ 5,  4,  7,  2,  3,  2,  0,  0],\n",
      "        [ 2,  2, 10,  5,  4,  2,  0,  0],\n",
      "        [ 2,  2,  9,  4,  2,  5,  0,  0],\n",
      "        [ 4, 10,  5,  3,  7,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 6,  8,  2,  9,  5,  3,  0,  0],\n",
      "        [ 9,  3,  5,  2, 10,  8,  0,  0],\n",
      "        [ 2, 10,  6,  8,  2,  5,  0,  0],\n",
      "        [ 2,  2,  2,  3,  2, 10,  0,  0],\n",
      "        [ 2,  2,  2,  5,  3,  2,  0,  0],\n",
      "        [ 2,  2,  3, 10,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 7,  2,  4, 10,  6,  2,  0,  0],\n",
      "        [ 2,  2,  2,  2,  4,  2,  0,  0],\n",
      "        [ 2,  2,  2,  6,  7,  2,  0,  0],\n",
      "        [10,  2,  7,  4,  2,  5,  0,  0],\n",
      "        [ 6,  2,  5,  9,  2,  4,  0,  0],\n",
      "        [ 2,  2, 10,  7,  5,  6,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2,  8,  3,  5, 10,  2,  0,  0],\n",
      "        [10,  2,  7,  8,  9,  3,  0,  0],\n",
      "        [ 2,  2,  5,  2,  7,  2,  0,  0],\n",
      "        [ 2,  2,  2,  9,  2,  5,  0,  0],\n",
      "        [ 2,  7,  9,  3,  5, 10,  0,  0],\n",
      "        [ 2,  2,  2,  2,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 5,  2,  2,  6,  2,  2,  0,  0],\n",
      "        [ 2,  6, 10,  8,  2,  5,  0,  0],\n",
      "        [ 2,  5,  8,  2,  6,  3,  0,  0],\n",
      "        [ 2,  2,  6,  5,  8,  2,  0,  0],\n",
      "        [ 2,  2,  2,  2,  7,  2,  0,  0],\n",
      "        [ 2,  2,  2, 10,  5,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[3, 9, 2, 2, 2, 5, 0, 0],\n",
      "        [8, 2, 2, 6, 2, 9, 0, 0],\n",
      "        [7, 2, 9, 2, 6, 3, 0, 0],\n",
      "        [6, 3, 2, 2, 7, 8, 0, 0],\n",
      "        [9, 7, 2, 2, 5, 6, 0, 0],\n",
      "        [5, 2, 2, 2, 2, 7, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[ 2,  4, 10,  2,  3,  2,  0,  0],\n",
      "        [ 9,  5,  2,  2, 10,  2,  0,  0],\n",
      "        [ 5,  2,  9,  2,  6,  3,  0,  0],\n",
      "        [ 2,  3,  2,  2,  9,  2,  0,  0],\n",
      "        [10,  6,  2,  3,  2,  2,  0,  0],\n",
      "        [ 3,  2,  4,  2,  2,  6,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2,  2,  2,  6,  9,  3,  0,  0],\n",
      "        [ 2,  9,  2,  2,  7,  2,  0,  0],\n",
      "        [ 7,  2,  2,  2,  2,  2,  0,  0],\n",
      "        [ 2,  4,  2,  7,  2, 10,  0,  0],\n",
      "        [ 4, 10,  9,  3,  2,  2,  0,  0],\n",
      "        [ 2,  6,  7,  2, 10,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 8, 10,  2,  5,  2,  2,  0,  0],\n",
      "        [ 2,  6,  5,  2,  2,  2,  0,  0],\n",
      "        [ 4,  2,  2,  2,  8,  5,  0,  0],\n",
      "        [ 2,  8, 10,  2,  2,  2,  0,  0],\n",
      "        [ 2,  2,  9,  8,  2,  6,  0,  0],\n",
      "        [ 6,  2,  2,  9,  4, 10,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[9, 2, 4, 2, 2, 2, 0, 0],\n",
      "        [3, 2, 8, 2, 2, 6, 0, 0],\n",
      "        [4, 2, 2, 6, 2, 3, 0, 0],\n",
      "        [8, 3, 2, 2, 2, 2, 0, 0],\n",
      "        [2, 8, 2, 2, 2, 4, 0, 0],\n",
      "        [7, 2, 2, 9, 6, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[ 2,  2,  2,  6, 10,  2,  0,  0],\n",
      "        [ 2,  2,  2,  3,  5,  7,  0,  0],\n",
      "        [ 2,  2,  2,  2,  6,  2,  0,  0],\n",
      "        [ 2,  6,  5,  9,  2, 10,  0,  0],\n",
      "        [10,  2,  3,  5,  9,  6,  0,  0],\n",
      "        [ 5,  2,  6,  2,  7,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[2, 2, 2, 5, 8, 2, 0, 0],\n",
      "        [5, 2, 2, 2, 9, 3, 0, 0],\n",
      "        [3, 9, 5, 6, 2, 8, 0, 0],\n",
      "        [6, 8, 2, 2, 5, 2, 0, 0],\n",
      "        [2, 2, 2, 2, 2, 2, 0, 0],\n",
      "        [2, 2, 3, 2, 2, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[ 5, 10,  6,  2,  9,  2,  0,  0],\n",
      "        [ 4,  2,  2,  2, 10,  5,  0,  0],\n",
      "        [ 2,  2, 10,  9,  2,  2,  0,  0],\n",
      "        [ 2,  2,  2, 10,  4,  2,  0,  0],\n",
      "        [10,  2,  2,  5,  6,  2,  0,  0],\n",
      "        [ 2,  2,  2,  4,  7, 10,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[3, 2, 2, 5, 2, 8, 0, 0],\n",
      "        [2, 2, 2, 7, 6, 3, 0, 0],\n",
      "        [2, 2, 2, 8, 5, 6, 0, 0],\n",
      "        [2, 8, 5, 3, 2, 4, 0, 0],\n",
      "        [5, 2, 3, 2, 8, 7, 0, 0],\n",
      "        [2, 2, 2, 4, 2, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[ 2,  2,  2,  6,  9,  2,  0,  0],\n",
      "        [ 2,  9,  2,  2,  2,  2,  0,  0],\n",
      "        [ 7,  4,  9,  2,  6,  2,  0,  0],\n",
      "        [ 2,  2,  2,  7,  4,  9,  0,  0],\n",
      "        [ 9,  7,  6,  2,  2,  2,  0,  0],\n",
      "        [ 2, 10,  2,  9,  7,  6,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[10,  2,  2,  4,  2,  2,  0,  0],\n",
      "        [ 2,  8,  2,  2,  7,  3,  0,  0],\n",
      "        [ 8,  7,  2,  2,  2,  2,  0,  0],\n",
      "        [ 2,  5, 10,  2,  4,  2,  0,  0],\n",
      "        [ 2,  4,  3,  7,  2,  2,  0,  0],\n",
      "        [ 2,  2,  2,  2,  3,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2, 10,  2,  6,  2,  8,  0,  0],\n",
      "        [ 2,  2,  2,  9,  5,  2,  0,  0],\n",
      "        [ 2,  2,  5,  3, 10,  2,  0,  0],\n",
      "        [10,  2,  2,  2,  2,  9,  0,  0],\n",
      "        [ 6,  2,  2,  2,  9,  2,  0,  0],\n",
      "        [ 9,  3,  2,  2,  6,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2,  2,  8,  2,  2,  7,  0,  0],\n",
      "        [ 7, 10,  3,  2,  5,  2,  0,  0],\n",
      "        [ 5,  2,  2,  2,  2,  2,  0,  0],\n",
      "        [10,  8,  7,  4,  3,  5,  0,  0],\n",
      "        [ 2,  2,  5, 10,  4,  2,  0,  0],\n",
      "        [ 8,  4,  2,  5,  2,  3,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[8, 9, 5, 6, 3, 4, 0, 0],\n",
      "        [2, 2, 2, 2, 5, 8, 0, 0],\n",
      "        [9, 2, 2, 5, 8, 2, 0, 0],\n",
      "        [2, 2, 2, 3, 2, 2, 0, 0],\n",
      "        [2, 2, 9, 8, 6, 2, 0, 0],\n",
      "        [2, 8, 2, 2, 2, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[ 6,  2,  3,  8,  2,  7,  0,  0],\n",
      "        [ 4,  7,  2,  3,  6, 10,  0,  0],\n",
      "        [ 2,  6,  7, 10,  2,  2,  0,  0],\n",
      "        [ 8,  4,  2,  2,  2,  2,  0,  0],\n",
      "        [10,  8,  2,  2,  2,  3,  0,  0],\n",
      "        [ 7,  3,  2,  6,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2,  7,  2,  2,  2,  3,  0,  0],\n",
      "        [ 2,  2,  9,  7,  4,  8,  0,  0],\n",
      "        [ 2,  3,  7,  8, 10,  4,  0,  0],\n",
      "        [ 4,  8, 10,  2,  2,  2,  0,  0],\n",
      "        [ 7,  2,  8,  2,  2,  2,  0,  0],\n",
      "        [10,  2,  2,  2,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[8, 2, 7, 9, 2, 6, 0, 0],\n",
      "        [2, 6, 2, 7, 2, 3, 0, 0],\n",
      "        [3, 9, 5, 2, 2, 7, 0, 0],\n",
      "        [6, 7, 2, 2, 3, 9, 0, 0],\n",
      "        [2, 8, 2, 3, 2, 2, 0, 0],\n",
      "        [7, 2, 2, 2, 9, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[ 2,  8,  2,  9,  4,  2,  0,  0],\n",
      "        [ 2,  4,  2,  3,  2, 10,  0,  0],\n",
      "        [ 2,  2,  3,  8,  2,  2,  0,  0],\n",
      "        [ 2,  2,  2,  2,  7,  3,  0,  0],\n",
      "        [ 7,  2,  8, 10,  9,  4,  0,  0],\n",
      "        [ 2,  9,  2,  7,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[8, 2, 3, 7, 9, 2, 0, 0],\n",
      "        [2, 2, 9, 2, 3, 6, 0, 0],\n",
      "        [2, 9, 2, 2, 2, 2, 0, 0],\n",
      "        [6, 2, 7, 2, 2, 2, 0, 0],\n",
      "        [9, 8, 6, 2, 4, 2, 0, 0],\n",
      "        [2, 7, 2, 2, 6, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[6, 2, 8, 2, 2, 2, 0, 0],\n",
      "        [2, 5, 2, 6, 8, 2, 0, 0],\n",
      "        [5, 2, 2, 2, 3, 2, 0, 0],\n",
      "        [2, 2, 2, 2, 2, 2, 0, 0],\n",
      "        [2, 6, 2, 8, 2, 3, 0, 0],\n",
      "        [2, 2, 2, 2, 5, 6, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "epoch 001 | batch 0000 | loss 82.6352 | exact 0.0000 | steps 16.00 | lr nan\n",
      "tensor([[ 2,  9,  4, 10,  7,  2,  0,  0],\n",
      "        [ 5,  2,  2,  2,  2,  3,  0,  0],\n",
      "        [ 7, 10,  2,  5,  2,  2,  0,  0],\n",
      "        [ 9,  2,  5,  3, 10,  2,  0,  0],\n",
      "        [ 2,  5,  7,  9,  2, 10,  0,  0],\n",
      "        [10,  3,  2,  7,  2,  4,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 3, 10,  2,  2,  7,  2,  0,  0],\n",
      "        [ 2,  6,  2,  2,  9,  2,  0,  0],\n",
      "        [ 2,  9,  3,  7,  8,  2,  0,  0],\n",
      "        [10,  7,  8,  9,  3,  6,  0,  0],\n",
      "        [ 9,  8,  2,  3,  2,  2,  0,  0],\n",
      "        [ 7,  2,  6,  2, 10,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[5, 2, 2, 6, 3, 2, 0, 0],\n",
      "        [6, 8, 3, 5, 7, 9, 0, 0],\n",
      "        [8, 2, 2, 2, 2, 2, 0, 0],\n",
      "        [9, 3, 6, 2, 2, 2, 0, 0],\n",
      "        [2, 2, 2, 2, 2, 5, 0, 0],\n",
      "        [3, 2, 2, 7, 9, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[ 2,  2,  2,  5,  8, 10,  0,  0],\n",
      "        [ 2,  2,  5,  2,  3,  2,  0,  0],\n",
      "        [ 9,  4,  2,  2,  2,  2,  0,  0],\n",
      "        [ 5, 10,  2,  2,  4,  2,  0,  0],\n",
      "        [ 2,  2,  2,  4,  5,  3,  0,  0],\n",
      "        [ 2,  2,  2,  2,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[3, 2, 2, 6, 2, 2, 0, 0],\n",
      "        [5, 2, 4, 2, 2, 2, 0, 0],\n",
      "        [2, 2, 2, 4, 2, 9, 0, 0],\n",
      "        [2, 2, 3, 7, 2, 2, 0, 0],\n",
      "        [2, 2, 2, 2, 2, 4, 0, 0],\n",
      "        [4, 5, 2, 2, 6, 7, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[ 2,  2,  5,  8,  3,  4,  0,  0],\n",
      "        [ 2,  2,  8,  2, 10,  2,  0,  0],\n",
      "        [ 2,  5,  2,  2,  2,  2,  0,  0],\n",
      "        [ 2,  2,  7,  4,  2, 10,  0,  0],\n",
      "        [ 2,  7,  2,  2,  2,  5,  0,  0],\n",
      "        [ 5,  4,  2, 10,  7,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  0,  0],\n",
      "        [ 5,  2,  3, 10,  4,  2,  0,  0],\n",
      "        [ 4,  3,  9,  2, 10,  6,  0,  0],\n",
      "        [ 6,  2,  2,  2,  3,  4,  0,  0],\n",
      "        [ 2,  2,  4,  6,  2, 10,  0,  0],\n",
      "        [ 2,  2,  2,  2,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 4,  2,  7,  2,  2, 10,  0,  0],\n",
      "        [ 5,  2, 10,  8,  2,  7,  0,  0],\n",
      "        [ 8,  2,  4,  2,  2,  2,  0,  0],\n",
      "        [ 2,  7,  2,  2,  2,  4,  0,  0],\n",
      "        [10,  5,  8,  2,  7,  6,  0,  0],\n",
      "        [ 2,  4,  2,  2,  2,  8,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 6,  2,  5,  8, 10,  2,  0,  0],\n",
      "        [ 2,  2,  2,  5,  2,  7,  0,  0],\n",
      "        [ 8,  6,  7,  3,  5,  2,  0,  0],\n",
      "        [ 5,  2,  3,  2,  2,  2,  0,  0],\n",
      "        [ 2,  5, 10,  2,  2,  8,  0,  0],\n",
      "        [ 3,  8,  6, 10,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[7, 4, 2, 3, 6, 8, 0, 0],\n",
      "        [2, 3, 2, 4, 2, 9, 0, 0],\n",
      "        [2, 7, 2, 2, 9, 2, 0, 0],\n",
      "        [9, 2, 2, 2, 2, 4, 0, 0],\n",
      "        [2, 8, 2, 9, 2, 2, 0, 0],\n",
      "        [2, 2, 6, 8, 3, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[3, 2, 7, 2, 2, 2, 0, 0],\n",
      "        [2, 2, 9, 3, 2, 7, 0, 0],\n",
      "        [2, 2, 2, 5, 2, 3, 0, 0],\n",
      "        [5, 2, 2, 2, 2, 8, 0, 0],\n",
      "        [2, 3, 2, 7, 2, 5, 0, 0],\n",
      "        [7, 2, 5, 2, 2, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[ 2,  6, 10,  2,  4,  7,  0,  0],\n",
      "        [ 7,  2,  4,  2,  2,  8,  0,  0],\n",
      "        [10,  2,  6,  2,  3,  4,  0,  0],\n",
      "        [ 2,  8,  3,  7,  6, 10,  0,  0],\n",
      "        [ 2,  2,  2,  2,  2,  2,  0,  0],\n",
      "        [ 6,  2,  2, 10,  2,  3,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2,  5,  2,  6,  2,  2,  0,  0],\n",
      "        [ 8,  2,  6,  3,  2,  2,  0,  0],\n",
      "        [ 6, 10,  2,  2,  3,  8,  0,  0],\n",
      "        [ 2,  8,  2,  2,  2,  2,  0,  0],\n",
      "        [ 5,  2,  2, 10,  2,  3,  0,  0],\n",
      "        [ 4,  2,  2,  2,  2,  6,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2,  7,  2,  2, 10,  8,  0,  0],\n",
      "        [ 2,  8,  3,  2,  2,  7,  0,  0],\n",
      "        [ 3,  5,  8,  2,  2,  2,  0,  0],\n",
      "        [ 9,  2,  7,  2,  3,  5,  0,  0],\n",
      "        [ 8,  2,  2, 10,  2,  2,  0,  0],\n",
      "        [ 2,  3,  2,  5,  8,  9,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[6, 2, 2, 5, 7, 9, 0, 0],\n",
      "        [2, 2, 2, 2, 2, 2, 0, 0],\n",
      "        [2, 2, 7, 2, 5, 6, 0, 0],\n",
      "        [5, 4, 2, 2, 8, 2, 0, 0],\n",
      "        [2, 2, 2, 6, 2, 5, 0, 0],\n",
      "        [2, 2, 2, 2, 4, 8, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[ 4,  3,  2,  2,  2,  2,  0,  0],\n",
      "        [10,  6,  7,  2,  3,  4,  0,  0],\n",
      "        [ 2,  2,  2,  2,  2,  6,  0,  0],\n",
      "        [ 2,  7,  2, 10,  8,  2,  0,  0],\n",
      "        [ 7,  2,  3,  6, 10,  2,  0,  0],\n",
      "        [ 8,  2,  2,  2,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2,  5, 10,  2,  2,  4,  0,  0],\n",
      "        [ 8,  2,  4,  3,  5,  2,  0,  0],\n",
      "        [ 2,  2,  2, 10,  2,  9,  0,  0],\n",
      "        [10,  3,  9,  4,  2,  2,  0,  0],\n",
      "        [ 9, 10,  2,  5,  2,  3,  0,  0],\n",
      "        [ 2,  4,  2,  2, 10,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2, 10,  8,  4,  2,  2,  0,  0],\n",
      "        [ 2,  4,  2, 10,  9,  8,  0,  0],\n",
      "        [ 6,  9,  2,  5,  8,  2,  0,  0],\n",
      "        [ 8,  2,  4,  9,  2,  2,  0,  0],\n",
      "        [10,  8,  5,  2,  4,  2,  0,  0],\n",
      "        [ 2,  2,  2,  8,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2,  6,  2,  2,  2,  9,  0,  0],\n",
      "        [ 2,  2,  8, 10,  2,  6,  0,  0],\n",
      "        [ 8,  2,  2,  7,  2,  5,  0,  0],\n",
      "        [ 2,  5,  6,  8,  9, 10,  0,  0],\n",
      "        [ 9,  7,  2,  2,  5,  2,  0,  0],\n",
      "        [ 6,  2,  5,  2,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2,  2,  3,  7,  2,  2,  0,  0],\n",
      "        [ 8,  4,  7,  3,  2,  2,  0,  0],\n",
      "        [ 2, 10,  9,  4,  2,  2,  0,  0],\n",
      "        [ 3,  2,  2,  9,  2,  7,  0,  0],\n",
      "        [ 4,  7,  2,  2,  2,  9,  0,  0],\n",
      "        [ 9,  2,  8,  2,  2,  4,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2,  2,  6,  2, 10,  2,  0,  0],\n",
      "        [ 2,  2,  2,  6,  2,  2,  0,  0],\n",
      "        [ 2,  2,  9,  2,  2, 10,  0,  0],\n",
      "        [ 2,  2,  3,  2,  5,  8,  0,  0],\n",
      "        [ 3,  5,  2,  2,  2,  2,  0,  0],\n",
      "        [ 9,  6,  8,  2,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[2, 2, 2, 5, 8, 2, 0, 0],\n",
      "        [2, 2, 8, 9, 2, 6, 0, 0],\n",
      "        [2, 8, 6, 2, 2, 2, 0, 0],\n",
      "        [9, 2, 2, 2, 6, 5, 0, 0],\n",
      "        [2, 3, 5, 2, 9, 8, 0, 0],\n",
      "        [2, 6, 9, 2, 2, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[2, 2, 3, 2, 9, 4, 0, 0],\n",
      "        [2, 4, 5, 3, 7, 6, 0, 0],\n",
      "        [2, 3, 2, 7, 2, 2, 0, 0],\n",
      "        [4, 2, 7, 2, 6, 2, 0, 0],\n",
      "        [5, 7, 2, 4, 3, 9, 0, 0],\n",
      "        [2, 9, 2, 2, 5, 7, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[ 2,  2,  9,  2,  2, 10,  0,  0],\n",
      "        [ 2,  2,  2,  4,  2,  2,  0,  0],\n",
      "        [ 4,  2,  7,  2,  2,  3,  0,  0],\n",
      "        [ 8,  9,  3,  2,  2,  2,  0,  0],\n",
      "        [ 9,  7,  4,  3, 10,  8,  0,  0],\n",
      "        [ 2,  3,  8,  7,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2,  6,  2,  2,  9,  2,  0,  0],\n",
      "        [ 7,  2,  2,  4,  6,  3,  0,  0],\n",
      "        [ 9,  2,  2,  2,  2,  2,  0,  0],\n",
      "        [ 2,  2,  7,  9,  2,  2,  0,  0],\n",
      "        [10,  4,  9,  3,  7,  6,  0,  0],\n",
      "        [ 3,  7,  6, 10,  4,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2,  2,  2,  8,  6,  2,  0,  0],\n",
      "        [ 2,  2,  2, 10,  4,  7,  0,  0],\n",
      "        [ 8,  2,  3,  2, 10,  2,  0,  0],\n",
      "        [ 2,  6,  2,  2,  2,  2,  0,  0],\n",
      "        [ 7,  8,  2,  2,  3,  2,  0,  0],\n",
      "        [ 2,  2,  4,  6,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[8, 2, 2, 5, 2, 4, 0, 0],\n",
      "        [6, 2, 2, 2, 8, 7, 0, 0],\n",
      "        [5, 8, 2, 4, 2, 6, 0, 0],\n",
      "        [2, 6, 4, 7, 2, 8, 0, 0],\n",
      "        [4, 2, 2, 2, 2, 3, 0, 0],\n",
      "        [7, 2, 6, 8, 4, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[ 7,  4,  2, 10,  2,  2,  0,  0],\n",
      "        [ 8,  2,  2,  5,  4,  2,  0,  0],\n",
      "        [ 2,  2,  2,  2,  5,  2,  0,  0],\n",
      "        [ 2,  2,  2,  2,  2,  4,  0,  0],\n",
      "        [ 5,  7,  2,  4,  8, 10,  0,  0],\n",
      "        [10,  2,  2,  2,  7,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 9,  7,  2,  2,  2,  2,  0,  0],\n",
      "        [ 2,  8,  2,  9,  2,  2,  0,  0],\n",
      "        [ 8,  4,  2,  7,  2, 10,  0,  0],\n",
      "        [ 2,  2,  2,  2,  9,  4,  0,  0],\n",
      "        [10,  2,  8,  2,  2,  9,  0,  0],\n",
      "        [ 4,  2,  7,  2,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[8, 3, 2, 9, 2, 4, 0, 0],\n",
      "        [2, 2, 2, 2, 2, 3, 0, 0],\n",
      "        [6, 9, 3, 2, 2, 2, 0, 0],\n",
      "        [2, 2, 4, 6, 2, 2, 0, 0],\n",
      "        [4, 6, 8, 3, 9, 2, 0, 0],\n",
      "        [3, 2, 9, 2, 8, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[ 2,  7,  6,  8,  2,  2,  0,  0],\n",
      "        [ 9,  8, 10,  2,  2,  6,  0,  0],\n",
      "        [ 2,  2,  8,  2,  2,  2,  0,  0],\n",
      "        [ 6,  9,  7,  4,  2,  8,  0,  0],\n",
      "        [ 8,  6,  2,  2,  7,  2,  0,  0],\n",
      "        [ 2, 10,  2,  6,  8,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "epoch 002 | batch 0000 | loss 81.7740 | exact nan | steps nan | lr nan\n",
      "tensor([[ 2,  2,  4,  5,  7,  2,  0,  0],\n",
      "        [ 2,  7,  2,  2,  2,  4,  0,  0],\n",
      "        [ 2,  4,  2,  2,  3,  2,  0,  0],\n",
      "        [ 2,  3,  2,  7,  4,  8,  0,  0],\n",
      "        [ 2,  2, 10,  2,  2,  2,  0,  0],\n",
      "        [ 3,  8,  7,  2,  5,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 3,  2, 10,  2,  4,  2,  0,  0],\n",
      "        [ 2,  2,  4,  2,  2,  9,  0,  0],\n",
      "        [ 2,  2,  8,  4,  9,  3,  0,  0],\n",
      "        [ 4,  3,  2,  2,  8,  2,  0,  0],\n",
      "        [ 8,  4,  2,  9,  2, 10,  0,  0],\n",
      "        [ 9, 10,  7,  2,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[10,  5,  6,  3,  2,  7,  0,  0],\n",
      "        [ 3,  7,  4, 10,  2,  5,  0,  0],\n",
      "        [ 2,  2,  5,  2,  2,  2,  0,  0],\n",
      "        [ 4, 10,  2,  2,  2,  3,  0,  0],\n",
      "        [ 7,  2, 10,  5,  2,  2,  0,  0],\n",
      "        [ 5,  4,  2,  2,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 5,  2,  3,  2,  6,  2,  0,  0],\n",
      "        [ 8,  6,  4,  3,  2,  2,  0,  0],\n",
      "        [ 3,  2,  6,  2,  2,  4,  0,  0],\n",
      "        [ 2,  2, 10,  6,  8,  2,  0,  0],\n",
      "        [10,  3,  8,  5,  4,  2,  0,  0],\n",
      "        [ 2,  4,  2,  8,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[2, 2, 6, 5, 2, 2, 0, 0],\n",
      "        [2, 4, 2, 8, 7, 2, 0, 0],\n",
      "        [7, 3, 2, 2, 2, 2, 0, 0],\n",
      "        [2, 2, 5, 2, 2, 8, 0, 0],\n",
      "        [2, 5, 7, 2, 8, 2, 0, 0],\n",
      "        [2, 2, 2, 3, 5, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[ 2,  7, 10,  9,  2,  2,  0,  0],\n",
      "        [ 6,  2,  2,  2,  4, 10,  0,  0],\n",
      "        [ 2,  2,  2,  2,  9,  2,  0,  0],\n",
      "        [ 2, 10,  4,  3,  2,  2,  0,  0],\n",
      "        [ 3,  4,  9,  2,  2,  7,  0,  0],\n",
      "        [ 2,  2,  2,  2,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 6, 10,  2,  4,  2,  2,  0,  0],\n",
      "        [ 9,  7,  2,  2,  6,  2,  0,  0],\n",
      "        [10,  4,  2,  9,  7,  2,  0,  0],\n",
      "        [ 7,  5,  2,  2, 10,  4,  0,  0],\n",
      "        [ 4,  2, 10,  2,  5,  2,  0,  0],\n",
      "        [ 2,  2,  7, 10,  2,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2,  2,  6,  2,  8,  2,  0,  0],\n",
      "        [ 2,  4,  2,  5,  6,  2,  0,  0],\n",
      "        [ 2,  5,  2,  2,  2,  2,  0,  0],\n",
      "        [ 2,  2,  9,  8,  4,  2,  0,  0],\n",
      "        [ 2,  2,  2,  2,  5,  6,  0,  0],\n",
      "        [ 2,  2,  2,  9, 10,  8,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2,  2,  2,  3,  2,  2,  0,  0],\n",
      "        [ 7,  8,  3,  2,  2,  6,  0,  0],\n",
      "        [ 2, 10,  7,  4,  2,  2,  0,  0],\n",
      "        [ 6,  2,  4,  7,  2, 10,  0,  0],\n",
      "        [10,  4,  2,  2,  2,  2,  0,  0],\n",
      "        [ 3,  7,  6,  2, 10,  4,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 9,  2,  6,  2,  4,  8,  0,  0],\n",
      "        [ 5,  2,  2,  2, 10,  6,  0,  0],\n",
      "        [ 2,  9,  2,  2,  5, 10,  0,  0],\n",
      "        [ 6,  5,  2,  2,  2,  4,  0,  0],\n",
      "        [ 2,  6,  9,  2,  8,  2,  0,  0],\n",
      "        [10,  8,  5,  2,  6,  9,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 5,  7,  2,  4,  2,  2,  0,  0],\n",
      "        [ 2,  2,  2,  2,  9,  5,  0,  0],\n",
      "        [10,  5,  2,  8,  4,  2,  0,  0],\n",
      "        [ 2,  2,  2,  2,  2,  2,  0,  0],\n",
      "        [ 2,  9,  8, 10,  2,  4,  0,  0],\n",
      "        [ 4, 10,  5,  2,  8,  7,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 9,  2,  2,  4,  8,  2,  0,  0],\n",
      "        [ 2,  4,  2,  5,  2,  2,  0,  0],\n",
      "        [ 8,  2,  5, 10,  2,  3,  0,  0],\n",
      "        [ 2,  2,  2,  2,  2,  2,  0,  0],\n",
      "        [ 2,  2,  2,  3, 10,  2,  0,  0],\n",
      "        [ 5,  3, 10,  8,  2,  4,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[2, 2, 2, 7, 9, 2, 0, 0],\n",
      "        [9, 2, 7, 2, 5, 2, 0, 0],\n",
      "        [3, 2, 2, 2, 8, 2, 0, 0],\n",
      "        [8, 2, 5, 2, 2, 2, 0, 0],\n",
      "        [6, 5, 3, 8, 7, 9, 0, 0],\n",
      "        [2, 2, 8, 2, 6, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[ 2,  2,  2,  6,  9,  2,  0,  0],\n",
      "        [ 2,  2,  2, 10,  8,  7,  0,  0],\n",
      "        [ 2,  2,  2,  2,  6,  2,  0,  0],\n",
      "        [ 2,  6,  8,  4,  2,  9,  0,  0],\n",
      "        [ 9,  2, 10,  8,  4,  6,  0,  0],\n",
      "        [ 8,  2,  6,  2,  7,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[2, 5, 7, 8, 9, 6, 0, 0],\n",
      "        [9, 8, 6, 2, 2, 2, 0, 0],\n",
      "        [8, 7, 2, 2, 5, 2, 0, 0],\n",
      "        [5, 6, 2, 7, 2, 4, 0, 0],\n",
      "        [7, 2, 5, 2, 2, 8, 0, 0],\n",
      "        [6, 4, 8, 2, 2, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[10,  6,  4,  2,  2,  2,  0,  0],\n",
      "        [ 7,  3,  5, 10,  6,  2,  0,  0],\n",
      "        [ 2,  5,  7,  3,  2, 10,  0,  0],\n",
      "        [ 2,  4, 10,  6,  2,  7,  0,  0],\n",
      "        [ 2,  2,  6,  5,  2,  2,  0,  0],\n",
      "        [ 5, 10,  2,  2,  2,  6,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[2, 2, 6, 8, 2, 4, 0, 0],\n",
      "        [8, 7, 2, 9, 5, 2, 0, 0],\n",
      "        [6, 8, 2, 4, 2, 7, 0, 0],\n",
      "        [2, 9, 2, 2, 8, 2, 0, 0],\n",
      "        [2, 6, 2, 5, 4, 8, 0, 0],\n",
      "        [2, 2, 2, 7, 2, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[2, 3, 5, 2, 2, 2, 0, 0],\n",
      "        [2, 2, 2, 3, 2, 4, 0, 0],\n",
      "        [9, 2, 3, 6, 8, 5, 0, 0],\n",
      "        [2, 6, 8, 4, 3, 2, 0, 0],\n",
      "        [2, 2, 6, 9, 4, 8, 0, 0],\n",
      "        [8, 2, 4, 5, 6, 2, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[ 5,  4, 10,  7,  8,  9,  0,  0],\n",
      "        [ 2,  8,  2,  2,  2,  2,  0,  0],\n",
      "        [10,  2,  4,  2,  5,  2,  0,  0],\n",
      "        [ 7,  5,  8,  2,  2, 10,  0,  0],\n",
      "        [ 2,  2,  2,  5,  2,  8,  0,  0],\n",
      "        [ 8, 10,  5,  2,  2,  4,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
      "tensor([[ 2,  2, 10,  6,  2,  2,  0,  0],\n",
      "        [ 2,  4,  2,  2, 10,  2,  0,  0],\n",
      "        [ 2,  6,  4,  2,  2,  2,  0,  0],\n",
      "        [ 7,  5,  2, 10,  4,  2,  0,  0],\n",
      "        [ 2,  2,  2,  2,  6,  2,  0,  0],\n",
      "        [ 2, 10,  2,  4,  5,  2,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[33m'\u001b[39m\u001b[33minput\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m].reshape(max_grid_size, max_grid_size))\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# If you want to reset carry every batch (instead of persisting), uncomment:\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# train_carry = None\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m train_carry, loss, logs = \u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_carry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_idx % print_every == \u001b[32m0\u001b[39m:\n\u001b[32m     21\u001b[39m     msg = (\n\u001b[32m     22\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[33m'\u001b[39m\u001b[33mtrain/loss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlr \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[33m'\u001b[39m\u001b[33mtrain/lr\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtraining_step\u001b[39m\u001b[34m(model, carry, batch, optimizer, total_steps, grad_clip)\u001b[39m\n\u001b[32m     13\u001b[39m     carry = model.initial_carry(batch)\n\u001b[32m     15\u001b[39m carry, loss, metrics, _ = model.compute_loss_and_metrics(carry, batch)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Warmup LR schedule (same logic)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bayesian-nano-trm/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bayesian-nano-trm/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bayesian-nano-trm/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "check_val_every_n_epoch = 10\n",
    "print_every = 50\n",
    "max_epochs = 100\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = steps_per_epoch * max_epochs\n",
    "train_carry = None\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        batch = move_batch_to_device(batch, device)\n",
    "        # If you want to reset carry every batch (instead of persisting), uncomment:\n",
    "        # train_carry = None\n",
    "\n",
    "        train_carry, loss, logs = training_step(\n",
    "            model, train_carry, batch, optimizer, total_steps=total_steps, grad_clip=1.0\n",
    "        )\n",
    "\n",
    "        if batch_idx % print_every == 0:\n",
    "            msg = (\n",
    "                f\"epoch {epoch:03d} | batch {batch_idx:04d} | \"\n",
    "                f\"loss {logs['train/loss']:.4f} | \"\n",
    "                f\"exact {logs.get('train/exact_accuracy', float('nan')):.4f} | \"\n",
    "                f\"steps {logs.get('train/steps', float('nan')):.2f} | \"\n",
    "                f\"lr {logs['train/lr']:.2e}\"\n",
    "            )\n",
    "            print(msg)\n",
    "    \"\"\"\n",
    "    if (epoch + 1) % check_val_every_n_epoch == 0:\n",
    "        val_logs = run_validation_epoch(model, val_loader, device)\n",
    "        print(\n",
    "            f\"[VAL @ epoch {epoch:03d}] \"\n",
    "            f\"exact={val_logs['val/exact_accuracy']:.4f} \"\n",
    "            f\"acc={val_logs['val/accuracy']:.4f} \"\n",
    "            f\"steps={val_logs['val/steps']:.2f} \"\n",
    "            f\"loss={val_logs['val/loss']:.4f}\"\n",
    "        )\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c980f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validation_step(model, batch):\n",
    "    \"\"\"\n",
    "    Mirrors your Lightning validation_step:\n",
    "    - fresh carry\n",
    "    - loop until all_halted\n",
    "    - accumulate metrics then average\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    batch_size = batch[\"input\"].shape[0]\n",
    "    carry = model.initial_carry(batch)\n",
    "\n",
    "    accumulated = {}\n",
    "    total_loss = 0.0\n",
    "    n_steps = 0\n",
    "\n",
    "    while True:\n",
    "        carry, loss, metrics, all_halted = model.compute_loss_and_metrics(carry, batch)\n",
    "\n",
    "        for k, v in metrics.items():\n",
    "            accumulated[k] = accumulated.get(k, 0.0) + float(v.item())\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        n_steps += 1\n",
    "\n",
    "        if bool(all_halted.item()):\n",
    "            break\n",
    "\n",
    "    count = accumulated.get(\"count\", float(batch_size))\n",
    "    if count > 0:\n",
    "        return {\n",
    "            \"val/loss\": total_loss / (n_steps * batch_size),\n",
    "            \"val/accuracy\": accumulated.get(\"accuracy\", 0.0) / count,\n",
    "            \"val/exact_accuracy\": accumulated.get(\"exact_accuracy\", 0.0) / count,\n",
    "            \"val/q_halt_accuracy\": accumulated.get(\"q_halt_accuracy\", 0.0) / count,\n",
    "            \"val/steps\": accumulated.get(\"steps\", 0.0) / count,\n",
    "            \"val/lm_loss\": accumulated.get(\"lm_loss\", 0.0) / (n_steps * batch_size),\n",
    "            \"val/q_halt_loss\": accumulated.get(\"q_halt_loss\", 0.0) / (n_steps * batch_size),\n",
    "        }\n",
    "    else:\n",
    "        return {k: 0.0 for k in [\"val/loss\",\"val/accuracy\",\"val/exact_accuracy\",\"val/q_halt_accuracy\",\"val/steps\",\"val/lm_loss\",\"val/q_halt_loss\"]}\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_validation_epoch(model, val_loader, device):\n",
    "    totals = {\n",
    "        \"val/loss\": 0.0,\n",
    "        \"val/accuracy\": 0.0,\n",
    "        \"val/exact_accuracy\": 0.0,\n",
    "        \"val/q_halt_accuracy\": 0.0,\n",
    "        \"val/steps\": 0.0,\n",
    "        \"val/lm_loss\": 0.0,\n",
    "        \"val/q_halt_loss\": 0.0,\n",
    "    }\n",
    "    n = 0\n",
    "    for batch in val_loader:\n",
    "        batch = move_batch_to_device(batch, device)\n",
    "        out = validation_step(model, batch)\n",
    "        for k in totals:\n",
    "            totals[k] += out[k]\n",
    "        n += 1\n",
    "    for k in totals:\n",
    "        totals[k] /= max(n, 1)\n",
    "    return totals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f445cdd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nano-trm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

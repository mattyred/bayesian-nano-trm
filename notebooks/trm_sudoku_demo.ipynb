{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1afdbda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "Imported AdamATan2 successfully\n"
     ]
    }
   ],
   "source": [
    "# conda env: scalinglaws\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.nn.modules.utils import compute_lr\n",
    "from src.nn.utils.constants import IGNORE_LABEL_ID\n",
    "from src.nn.optimizers.muon import Muon\n",
    "\n",
    "# pip install adam-atan2-pytorch\n",
    "try:\n",
    "    from adam_atan2_pytorch import AdamAtan2\n",
    "    print(f\"*\"*60)\n",
    "    print(\"Imported AdamATan2 successfully\")\n",
    "except ImportError:\n",
    "    print(\"Failed to import adam2\")\n",
    "\n",
    "from lightning import LightningModule\n",
    "\n",
    "from src.nn.modules.sparse_embeddings import (\n",
    "    CastedSparseEmbedding,\n",
    "    CastedSparseEmbeddingSignSGD_Distributed,\n",
    ")\n",
    "from src.nn.modules.trm_block import (\n",
    "    CastedEmbedding,\n",
    "    CastedLinear,\n",
    "    ReasoningBlock,\n",
    "    ReasoningBlockConfig,\n",
    "    ReasoningModule,\n",
    "    RotaryEmbedding,\n",
    "    RotaryEmbedding2D,\n",
    ")\n",
    "from src.nn.modules.utils import stablemax_cross_entropy, trunc_normal_init_\n",
    "from src.nn.utils import RankedLogger\n",
    "from src.nn.data.sudoku_datamodule import SudokuDataModule\n",
    "\n",
    "log = RankedLogger(__name__, rank_zero_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ad5bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sudoku_text(batch, idx=0, grid_size=6):\n",
    "    input_tensor = batch['input'][idx]\n",
    "    label_tensor = batch['output'][idx]\n",
    "    \n",
    "    # 1. We need to determine the max_grid_size used by the dataset to reshape correctly.\n",
    "    # We can infer it from the tensor length: sqrt(sequence_length)\n",
    "    seq_len = input_tensor.numel()\n",
    "    max_grid_size = int(seq_len**0.5)\n",
    "\n",
    "    # Box dimensions\n",
    "    if grid_size == 6: box_rows, box_cols = 2, 3\n",
    "    elif grid_size == 9: box_rows, box_cols = 3, 3\n",
    "    elif grid_size == 4: box_rows, box_cols = 2, 2\n",
    "    else: box_rows, box_cols = 2, 3\n",
    "\n",
    "    def decode_cell(val):\n",
    "        val = val.item()\n",
    "        if val == 2: return \".\"\n",
    "        if val > 2: return str(val - 2)\n",
    "        return \"?\" # 0=PAD or 1=EOS\n",
    "\n",
    "    def render_grid(tensor):\n",
    "        # CORRECTED LOGIC:\n",
    "        # 1. Reshape using the FULL max_grid_size to restore 2D structure\n",
    "        full_grid = tensor.reshape(max_grid_size, max_grid_size)\n",
    "        \n",
    "        # 2. Crop to the actual grid_size (top-left corner)\n",
    "        grid = full_grid[:grid_size, :grid_size]\n",
    "        \n",
    "        lines = []\n",
    "        dash_segment = \"-\" * (box_cols * 2 + 1)\n",
    "        h_sep = \"+\" + \"+\".join([dash_segment] * (grid_size // box_cols)) + \"+\"\n",
    "        \n",
    "        for r in range(grid_size):\n",
    "            if r % box_rows == 0:\n",
    "                lines.append(h_sep)\n",
    "            \n",
    "            row_str = \"|\"\n",
    "            for c in range(grid_size):\n",
    "                cell = decode_cell(grid[r, c])\n",
    "                row_str += f\" {cell}\"\n",
    "                if (c + 1) % box_cols == 0:\n",
    "                    row_str += \" |\"\n",
    "            lines.append(row_str)\n",
    "        lines.append(h_sep)\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    # Stats calculation (only on valid crop)\n",
    "    full_input_2d = input_tensor.reshape(max_grid_size, max_grid_size)\n",
    "    valid_input_crop = full_input_2d[:grid_size, :grid_size]\n",
    "    \n",
    "    givens = (valid_input_crop > 2).sum().item()\n",
    "    empty = (valid_input_crop == 2).sum().item()\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Sample {idx} (grid_size={grid_size}x{grid_size})\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Givens: {givens}, Empty: {empty}\\n\")\n",
    "    \n",
    "    print(\"Puzzle:\")\n",
    "    print(render_grid(input_tensor))\n",
    "    print(\"\\nSolution:\")\n",
    "    print(render_grid(label_tensor))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b8b4876",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TRMInnerCarry:\n",
    "    z_H: torch.Tensor\n",
    "    z_L: torch.Tensor\n",
    "\n",
    "@dataclass\n",
    "class TRMCarry:\n",
    "    inner_carry: TRMInnerCarry\n",
    "    steps: torch.Tensor\n",
    "    halted: torch.Tensor\n",
    "    current_data: Dict[str, torch.Tensor]\n",
    "\n",
    "class TRMModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 512,\n",
    "        num_layers: int = 2,\n",
    "        num_heads: int = 8,\n",
    "        max_grid_size: int = 30,\n",
    "        ffn_expansion: int = 2,\n",
    "        puzzle_emb_dim: int = 512,\n",
    "        puzzle_emb_len: int = 16,\n",
    "        pos_emb_type: str = \"1d\",\n",
    "        use_mlp_t: bool = False,\n",
    "        use_conv_swiglu: bool = False,\n",
    "        use_board_swiglu: bool = False,\n",
    "        vocab_size: int = 0,\n",
    "        num_puzzles: int = 0,\n",
    "        batch_size: int = 0,\n",
    "        pad_value: int = -1,\n",
    "        seq_len: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_value = pad_value\n",
    "        self.forward_dtype = torch.bfloat16\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Token embeddings\n",
    "        self.embed_scale = math.sqrt(self.hidden_size)\n",
    "        embed_init_std = 1.0 / self.embed_scale\n",
    "\n",
    "        # Input embedding\n",
    "        self.input_embedding = CastedEmbedding(\n",
    "            vocab_size, hidden_size, init_std=embed_init_std, cast_to=self.forward_dtype\n",
    "        )\n",
    "\n",
    "        # Puzzle embedding\n",
    "        if puzzle_emb_dim > 0:\n",
    "            self.puzzle_emb = CastedSparseEmbedding(\n",
    "                num_embeddings=num_puzzles,\n",
    "                embedding_dim=puzzle_emb_dim,\n",
    "                batch_size=batch_size,\n",
    "                init_std=0.0,\n",
    "                cast_to=self.forward_dtype,\n",
    "            )\n",
    "            self.puzzle_emb_len = puzzle_emb_len\n",
    "        else:\n",
    "            self.puzzle_emb = None\n",
    "            self.puzzle_emb_len = 0\n",
    "        \n",
    "        # Positional embeddings\n",
    "        if pos_emb_type == \"2d\":\n",
    "            self.pos_embedding = RotaryEmbedding2D(\n",
    "                dim=self.hidden_size // num_heads,\n",
    "                prefix_len=self.puzzle_emb_len, # Use self.puzzle_emb_len\n",
    "                max_grid_size=int(math.sqrt(self.seq_len)), \n",
    "                base=10000,\n",
    "            )\n",
    "        elif pos_emb_type == \"1d\":\n",
    "            self.pos_embedding = RotaryEmbedding(\n",
    "                dim=self.hidden_size // num_heads,\n",
    "                max_position_embeddings=self.seq_len + self.puzzle_emb_len, # Use self.puzzle_emb_len\n",
    "                base=10000,\n",
    "            )\n",
    "        \n",
    "        if not use_mlp_t:\n",
    "            assert pos_emb_type is not None, \"Rotary embeddings required if using attention\"\n",
    "\n",
    "        # Reasoning Block\n",
    "        reasoning_config = ReasoningBlockConfig(\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            expansion=ffn_expansion,\n",
    "            rms_norm_eps=1e-5,\n",
    "            seq_len=self.seq_len,\n",
    "            mlp_t=use_mlp_t,\n",
    "            puzzle_emb_ndim=puzzle_emb_dim,\n",
    "            puzzle_emb_len=self.puzzle_emb_len, # Use self.puzzle_emb_len\n",
    "            use_conv_swiglu=use_conv_swiglu,\n",
    "            use_board_swiglu=use_board_swiglu,\n",
    "            rows = max_grid_size,\n",
    "            cols = max_grid_size\n",
    "        )\n",
    "\n",
    "        self.lenet = ReasoningModule(\n",
    "            layers=[ReasoningBlock(reasoning_config) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.lm_head = CastedLinear(self.hidden_size, vocab_size, bias=False)\n",
    "        self.q_head = CastedLinear(self.hidden_size, 1, bias=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.q_head.weight.zero_()\n",
    "            if self.q_head.bias is not None:\n",
    "                self.q_head.bias.fill_(-5.0)\n",
    "\n",
    "        self.carry = None\n",
    "\n",
    "        self.z_H_init = nn.Buffer(\n",
    "            trunc_normal_init_(torch.empty(self.hidden_size, dtype=self.forward_dtype), std=1),\n",
    "            persistent=True,\n",
    "        )\n",
    "        self.z_L_init = nn.Buffer(\n",
    "            trunc_normal_init_(torch.empty(self.hidden_size, dtype=self.forward_dtype), std=1),\n",
    "            persistent=True,\n",
    "        )\n",
    "\n",
    "        # Add puzzle embeddings\n",
    "        if puzzle_emb_dim > 0:\n",
    "            self.puzzle_emb = CastedSparseEmbedding(\n",
    "                num_embeddings=num_puzzles,\n",
    "                embedding_dim=puzzle_emb_dim,\n",
    "                batch_size=batch_size,\n",
    "                init_std=0.0,  # Reference uses 0 init\n",
    "                cast_to=self.forward_dtype,\n",
    "            )\n",
    "            self.puzzle_emb_len = puzzle_emb_len\n",
    "            log.info(f\"Created puzzle_emb with num_puzzles={num_puzzles}, batch_size={batch_size}\")\n",
    "            log.info(f\"puzzle_emb.local_weights.shape: {self.puzzle_emb.local_weights.shape}\")\n",
    "            log.info(f\"puzzle_emb.weights.shape: {self.puzzle_emb.weights.shape}\")\n",
    "        else:\n",
    "            log.info(\"puzzle_emb_dim <= 0, not creating puzzle embeddings\")\n",
    "            self.puzzle_emb = None\n",
    "            self.puzzle_emb_len = 0\n",
    "\n",
    "    def _input_embeddings(self, input: torch.Tensor, puzzle_identifiers: torch.Tensor):\n",
    "        # Token embedding\n",
    "        embedding = self.input_embedding(input.to(torch.int32))\n",
    "\n",
    "        # Puzzle embeddings (Optional, based on your init)\n",
    "        if self.puzzle_emb is not None:\n",
    "            puzzle_embedding = self.puzzle_emb(puzzle_identifiers)\n",
    "            pad_count = self.puzzle_emb_len * self.hidden_size - puzzle_embedding.shape[-1]\n",
    "            if pad_count > 0:\n",
    "                puzzle_embedding = F.pad(puzzle_embedding, (0, pad_count))\n",
    "            \n",
    "            embedding = torch.cat(\n",
    "                (puzzle_embedding.view(-1, self.puzzle_emb_len, self.hidden_size), embedding),\n",
    "                dim=-2,\n",
    "            )\n",
    "\n",
    "        return self.embed_scale * embedding\n",
    "\n",
    "    def inner_forward(self, carry: TRMInnerCarry, batch: Dict[str, torch.Tensor]):\n",
    "        \"\"\"The core recurrent block: processes H_cycles and L_cycles.\"\"\"\n",
    "        \n",
    "        # Calculate Rotary Embeddings if available\n",
    "        seq_info = dict(\n",
    "            cos_sin=self.pos_embedding() if hasattr(self, \"pos_embedding\") else None,\n",
    "        )\n",
    "\n",
    "        input_embeddings = self._input_embeddings(batch[\"input\"], batch[\"puzzle_identifiers\"])\n",
    "\n",
    "        z_H, z_L = carry.z_H, carry.z_L\n",
    "        \n",
    "        # H_cycles: High-level reasoning\n",
    "        # We run H-1 cycles without gradients to save memory (standard TRM trick)\n",
    "        H_cycles = 3 # Default from your config\n",
    "        L_cycles = 6 # Default from your config\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(H_cycles - 1):\n",
    "                for _ in range(L_cycles):\n",
    "                    z_L = self.lenet(z_L, z_H + input_embeddings, **seq_info)\n",
    "                z_H = self.lenet(z_H, z_L, **seq_info)\n",
    "                \n",
    "        # The final cycle tracks gradients\n",
    "        for _ in range(L_cycles):\n",
    "            z_L = self.lenet(z_L, z_H + input_embeddings, **seq_info)\n",
    "        z_H = self.lenet(z_H, z_L, **seq_info)\n",
    "\n",
    "        # Output Heads\n",
    "        new_carry = TRMInnerCarry(z_H=z_H.detach(), z_L=z_L.detach())\n",
    "        \n",
    "        # Slicing off the puzzle embedding tokens to get just the grid predictions\n",
    "        output = self.lm_head(z_H)[:, self.puzzle_emb_len :] \n",
    "        q_logits = self.q_head(z_H[:, 0]).to(torch.float32)\n",
    "\n",
    "        return new_carry, output, q_logits[..., 0]\n",
    "\n",
    "    def initial_carry(self, batch_size, device):\n",
    "        \"\"\"Creates the initial zero-state carry.\"\"\"\n",
    "        return TRMInnerCarry(\n",
    "            z_H=torch.zeros(batch_size, self.seq_len + self.puzzle_emb_len, self.hidden_size, device=device, dtype=self.forward_dtype),\n",
    "            z_L=torch.zeros(batch_size, self.seq_len + self.puzzle_emb_len, self.hidden_size, device=device, dtype=self.forward_dtype),\n",
    "        )\n",
    "\n",
    "    def forward(self, carry: TRMCarry, batch: Dict[str, torch.Tensor], n_supervision: int):\n",
    "        \"\"\"\n",
    "        Runs one step of reasoning.\n",
    "        Logic: \n",
    "        1. If a sample in the batch is 'halted' (finished), reset its state (start over).\n",
    "        2. Run inner_forward.\n",
    "        3. Determine if we should halt now (based on steps or Q-head).\n",
    "        \"\"\"\n",
    "        batch_size = batch[\"input\"].shape[0]\n",
    "        device = batch[\"input\"].device\n",
    "\n",
    "        # If carry is None, initialize it (Start of a new batch)\n",
    "        if carry is None:\n",
    "            inner = self.initial_carry(batch_size, device)\n",
    "            carry = TRMCarry(\n",
    "                inner_carry=inner,\n",
    "                steps=torch.zeros((batch_size,), dtype=torch.int32, device=device),\n",
    "                halted=torch.ones((batch_size,), dtype=torch.bool, device=device), # Start as halted so we reset immediately\n",
    "                current_data=batch\n",
    "            )\n",
    "\n",
    "        # Reset logic: If a sequence halted in the *previous* step, reset it now to start fresh\n",
    "        # Note: We use z_H_init buffer from your init\n",
    "        reset_mask = carry.halted.view(-1, 1, 1)\n",
    "        new_z_H = torch.where(reset_mask, self.z_H_init, carry.inner_carry.z_H)\n",
    "        new_z_L = torch.where(reset_mask, self.z_L_init, carry.inner_carry.z_L)\n",
    "        \n",
    "        new_inner_carry = TRMInnerCarry(z_H=new_z_H, z_L=new_z_L)\n",
    "        new_steps = torch.where(carry.halted, 0, carry.steps)\n",
    "        \n",
    "        # Actual Forward Pass\n",
    "        new_inner_carry, logits, q_halt_logits = self.inner_forward(new_inner_carry, batch)\n",
    "\n",
    "        # Increment steps\n",
    "        new_steps = new_steps + 1\n",
    "        \n",
    "        # Halt Logic (Did we reach max supervision steps?)\n",
    "        # In a simple notebook version, we mostly rely on fixed N steps\n",
    "        halted = new_steps >= n_supervision\n",
    "        \n",
    "        # Update Carry\n",
    "        new_carry = TRMCarry(\n",
    "            inner_carry=new_inner_carry,\n",
    "            steps=new_steps,\n",
    "            halted=halted,\n",
    "            current_data=batch\n",
    "        )\n",
    "\n",
    "        return new_carry, logits, q_halt_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be681401",
   "metadata": {},
   "source": [
    "## Prepare Sudoku dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e769a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Ready: 6x6 Sudoku\n",
      "Vocab Size: 9, Sequence Length: 64\n"
     ]
    }
   ],
   "source": [
    "# 1. Instantiate the DataModule in Generation Mode (data_dir=None)\n",
    "grid_size = 6\n",
    "dm = SudokuDataModule(\n",
    "    data_dir=None,       \n",
    "    batch_size=32,       \n",
    "    num_train_puzzles=1000,\n",
    "    num_val_puzzles=100,\n",
    "    num_test_puzzles=100,\n",
    "    grid_size=grid_size,\n",
    "    num_workers=0  \n",
    ")\n",
    "\n",
    "# 2. Setup the data (generates the puzzle pool)\n",
    "dm.setup()\n",
    "\n",
    "# 3. specific loaders \n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "\n",
    "# 4. Extract metadata needed for the Model dimensions\n",
    "# In generation mode, these are calculated based on grid_size\n",
    "vocab_size = dm.vocab_size \n",
    "seq_len = dm.seq_len\n",
    "puzzle_emb_len = 16 # how many tokes for puzzle embedding\n",
    "\n",
    "print(f\"Data Ready: {grid_size}x{grid_size} Sudoku\")\n",
    "print(f\"Vocab Size: {vocab_size}, Sequence Length: {seq_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19ac373a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Sample 0 (grid_size=6x6)\n",
      "============================================================\n",
      "Givens: 18, Empty: 18\n",
      "\n",
      "Puzzle:\n",
      "+-------+-------+\n",
      "| 1 . . | 5 . 7 |\n",
      "| . . . | 4 3 1 |\n",
      "+-------+-------+\n",
      "| . . . | 7 5 3 |\n",
      "| . 7 5 | 1 . 8 |\n",
      "+-------+-------+\n",
      "| 5 . 1 | . 7 4 |\n",
      "| . . . | 8 . . |\n",
      "+-------+-------+\n",
      "\n",
      "Solution:\n",
      "+-------+-------+\n",
      "| 1 4 3 | 5 8 7 |\n",
      "| 7 5 8 | 4 3 1 |\n",
      "+-------+-------+\n",
      "| 8 1 4 | 7 5 3 |\n",
      "| 3 7 5 | 1 4 8 |\n",
      "+-------+-------+\n",
      "| 5 8 1 | 3 7 4 |\n",
      "| 4 3 7 | 8 1 5 |\n",
      "+-------+-------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "# Visualize the first element in the batch\n",
    "visualize_sudoku_text(batch, idx=0, grid_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b25c82",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "061b6c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_batch(batch, logits, grid_size=4):\n",
    "    \"\"\"Visualizes the first sample in the batch: Input vs Prediction vs Target\"\"\"\n",
    "    inputs = batch['input'][0].cpu().numpy()\n",
    "    targets = batch['output'][0].cpu().numpy()\n",
    "    preds = logits[0].argmax(dim=-1).cpu().numpy()\n",
    "    \n",
    "    # Remap tokens back to numbers (0=pad, 1=eos, 2=empty, 3+=values)\n",
    "    # See pad_and_encode in SudokuDataset\n",
    "    def decode(flat_arr):\n",
    "        arr = flat_arr[:grid_size*grid_size].reshape(grid_size, grid_size)\n",
    "        # Shift back: 2->0 (empty), 3->1 (val 1)\n",
    "        res = np.zeros_like(arr)\n",
    "        mask = arr >= 3\n",
    "        res[mask] = arr[mask] - 2\n",
    "        return res\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 4))\n",
    "    titles = [\"Input (0=Empty)\", \"Prediction\", \"Target\"]\n",
    "    for ax, data, title in zip(axes, [inputs, preds, targets], titles):\n",
    "        grid = decode(data)\n",
    "        ax.matshow(grid, cmap='Blues')\n",
    "        for (i, j), z in np.ndenumerate(grid):\n",
    "            ax.text(j, i, f'{z}', ha='center', va='center', \n",
    "                    color='black' if z != 0 else 'lightgray')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69199f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TRM Model...\n",
      "----------------------------------------\n",
      "Model successfully initialized on cpu\n",
      "Parameters: 4.83M\n",
      "Configuration: MLP_T=True, PosEmb=None\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"grid_size\": 6,             \n",
    "    \"batch_size\": 32,\n",
    "    \"max_epochs\": 1000,\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_heads\": 8,\n",
    "    \"ffn_expansion\": 4,\n",
    "    \"puzzle_emb_dim\": 0,        \n",
    "    \"puzzle_emb_len\": 0,        \n",
    "    \"pos_emb_type\": None,       \n",
    "    \"use_mlp_t\": True,          \n",
    "    \"use_conv_swiglu\": False,\n",
    "    \"use_board_swiglu\": False,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1.0,\n",
    "    \"N_supervision\": 16\n",
    "}\n",
    "\n",
    "device = \"cpu\" #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Initializing TRM Model...\")\n",
    "\n",
    "model = TRMModel(\n",
    "    vocab_size=3 + dm.max_grid_size,  # 0=PAD, 1=EOS, 2=Empty, 3+=Values\n",
    "    seq_len=dm.seq_len,\n",
    "    max_grid_size=dm.max_grid_size,\n",
    "    pad_value=0,\n",
    "    hidden_size=CONFIG[\"hidden_size\"],\n",
    "    num_layers=CONFIG[\"num_layers\"],\n",
    "    num_heads=CONFIG[\"num_heads\"],\n",
    "    ffn_expansion=CONFIG[\"ffn_expansion\"],\n",
    "    puzzle_emb_dim=CONFIG[\"puzzle_emb_dim\"],\n",
    "    puzzle_emb_len=CONFIG[\"puzzle_emb_len\"],\n",
    "    pos_emb_type=CONFIG[\"pos_emb_type\"],  \n",
    "    use_mlp_t=CONFIG[\"use_mlp_t\"],    \n",
    "    use_conv_swiglu=CONFIG[\"use_conv_swiglu\"],\n",
    "    use_board_swiglu=CONFIG[\"use_board_swiglu\"],\n",
    "    # Dataset specific (required for safeguards even if puzzle_emb_dim=0)\n",
    "    num_puzzles=0,    \n",
    "    batch_size=CONFIG[\"batch_size\"]\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=CONFIG[\"learning_rate\"], \n",
    "    weight_decay=CONFIG[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Model successfully initialized on {device}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "print(f\"Configuration: MLP_T={CONFIG['use_mlp_t']}, PosEmb={CONFIG['pos_emb_type']}\")\n",
    "print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa5806f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cpu...\n",
      "============================================================\n",
      "Sample 0 (grid_size=6x6)\n",
      "============================================================\n",
      "Givens: 16, Empty: 20\n",
      "\n",
      "Puzzle:\n",
      "+-------+-------+\n",
      "| 7 4 . | 8 . 1 |\n",
      "| . . . | . 7 . |\n",
      "+-------+-------+\n",
      "| 1 . . | 2 . . |\n",
      "| 4 8 2 | 7 . 6 |\n",
      "+-------+-------+\n",
      "| . . 4 | . 8 7 |\n",
      "| . . . | 4 . . |\n",
      "+-------+-------+\n",
      "\n",
      "Solution:\n",
      "+-------+-------+\n",
      "| 7 4 6 | 8 2 1 |\n",
      "| 2 1 8 | 6 7 4 |\n",
      "+-------+-------+\n",
      "| 1 6 7 | 2 4 8 |\n",
      "| 4 8 2 | 7 1 6 |\n",
      "+-------+-------+\n",
      "| 6 2 4 | 1 8 7 |\n",
      "| 8 7 1 | 4 6 2 |\n",
      "+-------+-------+\n",
      "\n",
      "\n",
      "tensor([ 9,  6,  2, 10,  2,  3,  0,  0,  2,  2,  2,  2,  9,  2,  0,  0,  3,  2,\n",
      "         2,  4,  2,  2,  0,  0,  6, 10,  4,  9,  2,  8,  0,  0,  2,  2,  6,  2,\n",
      "        10,  9,  0,  0,  2,  2,  2,  6,  2,  2,  0,  0,  1,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([   9,    6,    8,   10,    4,    3, -100, -100,    4,    3,   10,    8,\n",
      "           9,    6, -100, -100,    3,    8,    9,    4,    6,   10, -100, -100,\n",
      "           6,   10,    4,    9,    3,    8, -100, -100,    8,    4,    6,    3,\n",
      "          10,    9, -100, -100,   10,    9,    3,    6,    8,    4, -100, -100,\n",
      "           1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m optimizer.zero_grad()\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m carry, logits, q_logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcarry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_supervision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_SUPERVISION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Compute Loss\u001b[39;00m\n\u001b[32m     27\u001b[39m labels = batch[\u001b[33m'\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ibex/user/rossom/conda-environments/scalinglaws/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ibex/user/rossom/conda-environments/scalinglaws/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 232\u001b[39m, in \u001b[36mTRMModel.forward\u001b[39m\u001b[34m(self, carry, batch, n_supervision)\u001b[39m\n\u001b[32m    229\u001b[39m new_steps = torch.where(carry.halted, \u001b[32m0\u001b[39m, carry.steps)\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# Actual Forward Pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m new_inner_carry, logits, q_halt_logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minner_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inner_carry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# Increment steps\u001b[39;00m\n\u001b[32m    235\u001b[39m new_steps = new_steps + \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 177\u001b[39m, in \u001b[36mTRMModel.inner_forward\u001b[39m\u001b[34m(self, carry, batch)\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(H_cycles - \u001b[32m1\u001b[39m):\n\u001b[32m    176\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(L_cycles):\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m             z_L = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_L\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_H\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mseq_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m         z_H = \u001b[38;5;28mself\u001b[39m.lenet(z_H, z_L, **seq_info)\n\u001b[32m    180\u001b[39m \u001b[38;5;66;03m# The final cycle tracks gradients\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ibex/user/rossom/conda-environments/scalinglaws/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ibex/user/rossom/conda-environments/scalinglaws/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ibex/user/rossom/bayesian-nano-trm/src/nn/modules/trm_block.py:485\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, hidden_states, input_injection, **kwargs)\u001b[39m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m    482\u001b[39m     \u001b[38;5;28mself\u001b[39m.layers = torch.nn.ModuleList(layers)\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     \u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_injection: torch.Tensor, **kwargs\n\u001b[32m    486\u001b[39m ) -> torch.Tensor:\n\u001b[32m    487\u001b[39m     hidden_states = hidden_states + input_injection\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ibex/user/rossom/conda-environments/scalinglaws/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ibex/user/rossom/conda-environments/scalinglaws/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ibex/user/rossom/bayesian-nano-trm/src/nn/modules/trm_block.py:470\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, cos_sin, hidden_states)\u001b[39m\n\u001b[32m    468\u001b[39m     hidden_states = rms_norm(hidden_states + out, variance_epsilon=self.norm_eps)\n\u001b[32m    469\u001b[39m     hidden_states = hidden_states.transpose(1,2)\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m else:\n\u001b[32m    471\u001b[39m     attn_out = self.self_attn(cos_sin=cos_sin, hidden_states=hidden_states)\n\u001b[32m    472\u001b[39m     hidden_states = rms_norm(hidden_states + attn_out, variance_epsilon=self.norm_eps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ibex/user/rossom/conda-environments/scalinglaws/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ibex/user/rossom/conda-environments/scalinglaws/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ibex/user/rossom/bayesian-nano-trm/src/nn/modules/trm_block.py:189\u001b[39m, in \u001b[36mSwiGLU.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     gate, up = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_up_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.chunk(\u001b[32m2\u001b[39m, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.down_proj(F.silu(gate) * up)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ibex/user/rossom/conda-environments/scalinglaws/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ibex/user/rossom/conda-environments/scalinglaws/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ibex/user/rossom/bayesian-nano-trm/src/nn/modules/trm_block.py:46\u001b[39m, in \u001b[36mCastedLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(f\"Starting training on {device}...\")\n",
    "EPOCHS = 3\n",
    "N_SUPERVISION = 4\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Move batch to deviceÃŸ\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        visualize_sudoku_text(batch, idx=0, grid_size=6)\n",
    "        print(batch['input'][0])\n",
    "        print(batch['output'][0])\n",
    "        # 1. Initialize State for this new batch\n",
    "        carry = None \n",
    "        \n",
    "        # 2. Reasoning Loop (The \"Recurrent\" part of TRM)\n",
    "        # We process the SAME batch multiple times (N_SUPERVISION)\n",
    "        # allowing the model to refine its hidden state\n",
    "        for step in range(N_SUPERVISION):\n",
    "            optimizer.zero_grad()\n",
    "            # 1. Forward pass\n",
    "            carry, logits, q_logits = model(carry, batch, n_supervision=N_SUPERVISION)\n",
    "            \n",
    "            labels = batch['output']\n",
    "            \n",
    "            # ---------------------------------------------------------\n",
    "            # Loss 1: Language Modeling (Sudoku Solution)\n",
    "            # ---------------------------------------------------------\n",
    "            # Flatten for CrossEntropy: [batch * seq_len, vocab_size]\n",
    "            flat_logits = logits.reshape(-1, logits.shape[-1])\n",
    "            flat_labels = labels.reshape(-1)\n",
    "\n",
    "            lm_loss = F.cross_entropy(\n",
    "                flat_logits, \n",
    "                flat_labels, \n",
    "                ignore_index=-100 \n",
    "            )\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # Loss 2: Halting (Q-Head)\n",
    "            # ---------------------------------------------------------\n",
    "            # We need to determine if the model *actually* got the solution right \n",
    "            # for this specific step. The target for the Q-head is 1.0 if correct, 0.0 otherwise.\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Get predictions\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                \n",
    "                # Mask out padding/ignore tokens (-100)\n",
    "                mask = labels != -100\n",
    "                \n",
    "                # Check correctness per cell\n",
    "                # (Where mask is True, pred must match label)\n",
    "                correct_cells = (preds == labels) & mask\n",
    "                \n",
    "                # Count required correct cells per sequence\n",
    "                required_correct = mask.sum(dim=-1)\n",
    "                \n",
    "                # Count actual correct cells per sequence\n",
    "                actual_correct = correct_cells.sum(dim=-1)\n",
    "                \n",
    "                # A sequence is correct ONLY if all non-ignored tokens match\n",
    "                seq_is_correct = (actual_correct == required_correct).float()\n",
    "\n",
    "            # Binary Cross Entropy for the halting head\n",
    "            # q_logits shape: [batch_size] -> We need to align with seq_is_correct\n",
    "            q_halt_loss = F.binary_cross_entropy_with_logits(\n",
    "                q_logits, \n",
    "                seq_is_correct.to(q_logits.device)\n",
    "            )\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # Total Loss\n",
    "            # ---------------------------------------------------------\n",
    "            # Standard TRM weighting: 1.0 * LM + 0.5 * Q_Halt\n",
    "            loss = lm_loss + 0.5 * q_halt_loss\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Detach carry to prevent infinite graph growth\n",
    "            if carry is not None:\n",
    "                carry.inner_carry.z_H = carry.inner_carry.z_H.detach()\n",
    "                carry.inner_carry.z_L = carry.inner_carry.z_L.detach()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
